{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99083e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "original_cwd = os.getcwd()\n",
    "\n",
    "backend_path = os.path.abspath(os.path.join(original_cwd, \"../..\"))\n",
    "print(backend_path)\n",
    "print(sys.path)\n",
    "\n",
    "if backend_path not in sys.path:\n",
    "    sys.path.insert(0, backend_path)\n",
    "\n",
    "from shared.snowflake.client import SnowflakeClient\n",
    "from experiments.utils.llm import build_prompt, get_llm_response, compare_ground_truth_vs_llm, count_number_tokens, split_docs_recursively\n",
    "\n",
    "print(f\"Returned to original working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c88417b",
   "metadata": {},
   "source": [
    "**load the config file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2208ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"config\"):\n",
    "    os.chdir(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "CONFIG_FILE_PATH = \"config/base_config.json\"\n",
    "\n",
    "with open(CONFIG_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7a230",
   "metadata": {},
   "source": [
    "**set the experience id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23fa4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIENCE_ID = config[\"experiments_specifique_params\"][\"experiment_id\"]\n",
    "\n",
    "print(EXPERIENCE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c13ca",
   "metadata": {},
   "source": [
    "**load the embedding file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9260510",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FILE = config[\"input_recipies_file\"]\n",
    "\n",
    "df_recipes_cleaned = pd.read_csv(INPUT_DATA_FILE)\n",
    "\n",
    "df_recipes_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19e6462",
   "metadata": {},
   "source": [
    "## evaluation the LLM and the prompt system using the ground truth ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee59014",
   "metadata": {},
   "source": [
    "**load the test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eea0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_TEST_FILE_PATH = config[\"query_test_file_path\"]\n",
    "\n",
    "with open(QUERY_TEST_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    ground_truth_results = json.load(f)\n",
    "\n",
    "print(QUERY_TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682456b",
   "metadata": {},
   "source": [
    "**load the prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3526944",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_EVAL_PATH = config[\"eval_prompt_file\"]\n",
    "\n",
    "with open(PROMPT_EVAL_PATH, \"r\") as f:\n",
    "    prompt_template = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aadd07",
   "metadata": {},
   "source": [
    "**calculate the number of tokens in the prompt template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e1a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = config[\"llm_model\"]\n",
    "LLM_MODEL_CONTEXT_WINDOWS = config[\"context_window\"]\n",
    "LLM_TEMPERATURE = config[\"temperature\"]\n",
    "LLM_MAX_TOKENS = config['max_tokens']\n",
    "\n",
    "num_token_prompt_template = count_number_tokens(client=SnowflakeClient(), model=LLM_MODEL, text=str(prompt_template))\n",
    "num_token_prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11375320",
   "metadata": {},
   "source": [
    "**define the schema of the response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2274d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON schema for recipe relevance ratings\n",
    "json_schema = config['llm_json_format_without_justification']\n",
    "json_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1cb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TEXT = config[\"columns_to_clean\"]\n",
    "\n",
    "llm_results = []\n",
    "\n",
    "for query in ground_truth_results:\n",
    "    query_text = query[\"query_text\"]\n",
    "    num_token_query = count_number_tokens(client=SnowflakeClient(), model=LLM_MODEL, text=str(query_text))\n",
    "    \n",
    "    # Build doc entries for the prompt\n",
    "    doc_entries = []\n",
    "    for document in query[\"relevance_documents\"]:\n",
    "        doc_id = document['doc_id']\n",
    "        recipe_row = df_recipes_cleaned[df_recipes_cleaned[\"ID\"] == doc_id]\n",
    "        if recipe_row.empty:\n",
    "            continue\n",
    "\n",
    "        recipe_info = {}\n",
    "        for col_key, col_info in COLUMNS_TEXT.items():\n",
    "            recipe_info[col_info[\"start_text\"]] = recipe_row.iloc[0][col_info[\"column_name\"]]\n",
    "\n",
    "        doc_entries.append({\"doc_id\": doc_id, \"recipe_info\": recipe_info})\n",
    "    \n",
    "    num_token_doc = count_number_tokens(client=SnowflakeClient(), model=LLM_MODEL, text=str(doc_entries))\n",
    "    \n",
    "    doc_entries_list = split_docs_recursively(client=SnowflakeClient(), model=LLM_MODEL, max_tokens=LLM_MODEL_CONTEXT_WINDOWS, base_template_size=num_token_prompt_template, num_token_query=num_token_query, num_token_doc=num_token_doc, doc_entries=doc_entries)\n",
    "    \n",
    "    for doc_batch in doc_entries_list:\n",
    "        prompt = build_prompt(prompt_template, query_text, doc_entries)\n",
    "\n",
    "        # Call the LLM\n",
    "        llm_response = get_llm_response(client=SnowflakeClient(), model=LLM_MODEL, prompt=prompt, response_format=json_schema, temperature=LLM_TEMPERATURE, max_tokens=LLM_MAX_TOKENS)\n",
    "\n",
    "        # Extract and parse JSON (this now returns a proper dict, not a string)\n",
    "        json_output = json.loads(llm_response)\n",
    "        print(json_output)\n",
    "    \n",
    "        llm_results.append(json_output)\n",
    "        \n",
    "    print(f\"âœ“ Parsed JSON for query: {query_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f749477",
   "metadata": {},
   "source": [
    "## compare LLM response to the ground truth ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602b3d5",
   "metadata": {},
   "source": [
    "**compare the LLM response with the ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_avg_query, coherence_per_query = compare_ground_truth_vs_llm(ground_truth_results, llm_results)\n",
    "\n",
    "QUERY_LLM_RESULTS_PATH = config['query_llm_file_path'].format(\n",
    "    experiment_id=EXPERIENCE_ID \n",
    ")\n",
    "\n",
    "print(\"Final coherence score (avg query):\", coherence_avg_query)\n",
    "print(\"Per-query coherence:\", coherence_per_query)\n",
    "\n",
    "llm_results.append({'COHERENCE_SCORE_AVG_QUERY': coherence_avg_query})\n",
    "\n",
    "# Save results - now they're proper dicts, not strings\n",
    "with open(QUERY_LLM_RESULTS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(llm_results, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d964498",
   "metadata": {},
   "source": [
    "**write the config file for that experience**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529a5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUPUT_EXPERIMENT_DIR = config[\"output_experiments_dir\"].format(\n",
    "    experiment_id=EXPERIENCE_ID \n",
    ")\n",
    "\n",
    "# Write the config file\n",
    "with open(os.path.join(OUPUT_EXPERIMENT_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "    \n",
    "# Write the prompt file\n",
    "with open(os.path.join(OUPUT_EXPERIMENT_DIR, \"eval_prompt.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(prompt_template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
