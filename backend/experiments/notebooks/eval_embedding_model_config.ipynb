{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "original_cwd = os.getcwd()\n",
    "\n",
    "backend_path = os.path.abspath(os.path.join(original_cwd, \"../..\"))\n",
    "print(backend_path)\n",
    "print(sys.path)\n",
    "\n",
    "if backend_path not in sys.path:\n",
    "    sys.path.insert(0, backend_path)\n",
    "\n",
    "from shared.snowflake.client import SnowflakeClient\n",
    "from experiments.utils.llm import build_prompt, get_llm_response, compare_ground_truth_vs_llm, count_number_tokens, split_docs_recursively\n",
    "\n",
    "print(f\"Returned to original working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db299b4",
   "metadata": {},
   "source": [
    "**laod the config file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62254183",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"config\"):\n",
    "    os.chdir(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "CONFIG_FILE_PATH = \"config/config.json\"\n",
    "\n",
    "with open(CONFIG_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7a3300",
   "metadata": {},
   "source": [
    "**set the experience id** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82719237",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIENCE_ID = config[\"experiments_specifique_params\"][\"experiment_id\"]\n",
    "\n",
    "print(EXPERIENCE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdab1de1",
   "metadata": {},
   "source": [
    "**load the embedding file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daaa4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_EMBEDDINGS_FILE = config[\"output_recipies_embedding_file\"].format(\n",
    "    experiment_id=EXPERIENCE_ID\n",
    ")\n",
    "df_recipes_cleaned = pd.read_csv(INPUT_EMBEDDINGS_FILE)\n",
    "\n",
    "\n",
    "emb_columns = [col for col in df_recipes_cleaned.columns if col.endswith('_EMB')]\n",
    "print(f\"Found embedding columns: {emb_columns}\")\n",
    "\n",
    "for col in emb_columns:\n",
    "    df_recipes_cleaned[col] = df_recipes_cleaned[col].apply(\n",
    "        lambda x: np.fromstring(x.strip('[]'), sep=' ', dtype=np.float32)\n",
    "    )\n",
    "\n",
    "for col in emb_columns:\n",
    "    print(f\"{col} -> first embedding shape: {df_recipes_cleaned[col][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d42f4",
   "metadata": {},
   "source": [
    "**load the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb256fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "MODELS_CONFIG = config[\"models\"]\n",
    "COLUMNS_TO_EMBEDDE = config[\"columns_embedding\"]\n",
    "\n",
    "#create a dict {name model : model} \n",
    "MODELS_LIST = [SentenceTransformer(model_id) for model_id in MODELS_CONFIG]\n",
    "MODEL_DICT = dict(zip(MODELS_CONFIG, MODELS_LIST))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3467d",
   "metadata": {},
   "source": [
    "**define function to retrive documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c181c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(\n",
    "    query: str,\n",
    "    model: SentenceTransformer,\n",
    "    documents: list,\n",
    "    df: pd.DataFrame,\n",
    "    columns_to_select: list,\n",
    "    top_k: int\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve top-k documents relevant to the query using the specified embedding model.\n",
    "\n",
    "    Returns:\n",
    "        list: [{col1: ..., col2: ..., similarity_score: ...}, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True, device=device)\n",
    "    query_embedding = normalize(query_embedding, p=2, dim=1)\n",
    "\n",
    "    # Build document embedding tensor\n",
    "    document_embeddings = torch.stack(\n",
    "        [torch.tensor(doc, dtype=query_embedding.dtype, device=device) for doc in documents]\n",
    "    )\n",
    "    document_embeddings = normalize(document_embeddings, p=2, dim=1)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_scores = torch.matmul(query_embedding, document_embeddings.T).squeeze(0)\n",
    "\n",
    "    # Top-k results\n",
    "    top_k_results = torch.topk(cosine_scores, k=top_k)\n",
    "    top_k_indices = top_k_results.indices.tolist()\n",
    "    top_k_scores = top_k_results.values.tolist()\n",
    "\n",
    "     # Select rows & columns\n",
    "    retrieved_df = df.iloc[top_k_indices][columns_to_select].copy()\n",
    "\n",
    "    # Add similarity scores\n",
    "    retrieved_df[\"similarity_score\"] = top_k_scores\n",
    "\n",
    "    # Return list of documents\n",
    "    return retrieved_df.to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619eaeb",
   "metadata": {},
   "source": [
    "**get the top_k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb0e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K_LIST = config[\"experiments_specifique_params\"][\"top_k\"]\n",
    "\n",
    "TOP_K_LIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc9b0c",
   "metadata": {},
   "source": [
    "**Get test queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c76928",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_test_file_path = config[\"query_test_file_path\"]\n",
    "\n",
    "with open(queries_test_file_path, 'r') as file:\n",
    "    QUERIES_TEST_LIST = json.load(file)\n",
    "    \n",
    "QUERIES_TEST_LIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ada403",
   "metadata": {},
   "source": [
    "**Get list of columns to select from the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMNS = config['data_columns']\n",
    "DATA_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d450e4c",
   "metadata": {},
   "source": [
    "**For each top_k, model, embedding config, queries assign the doc retrived**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade0fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_DATA_DIR = config[\"output_retrieved_documents_file\"].format(experiment_id=EXPERIENCE_ID)\n",
    "# os.makedirs(OUTPUT_DATA_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_RETRIEVED_DOCS_FILE = config[\"output_retrieved_documents_file\"].format(\n",
    "    experiment_id=EXPERIENCE_ID    \n",
    ")\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(OUTPUT_RETRIEVED_DOCS_FILE):\n",
    "    print(f\"Loading cached results from {OUTPUT_RETRIEVED_DOCS_FILE} ...\")\n",
    "    with open(OUTPUT_RETRIEVED_DOCS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        retrived_model_query_docs_dict = json.load(f)\n",
    "else:\n",
    "    print(\"File not found. Running retrieval calculations ...\")\n",
    "\n",
    "    EMB_COLS = [col for col in df_recipes_cleaned.columns if col.endswith(\"_EMB\")]\n",
    "    retrived_model_query_docs_dict = {}\n",
    "\n",
    "    for k in TOP_K_LIST:\n",
    "        for emb_col in EMB_COLS:\n",
    "            for query in QUERIES_TEST_LIST:\n",
    "\n",
    "                # Convert embedding column to a list of numpy arrays\n",
    "                documents = df_recipes_cleaned[emb_col].apply(np.array).to_list()\n",
    "\n",
    "                # Build model name + config name from column name\n",
    "                model_name = \"/\".join(emb_col.split(\"/\")[:-1])\n",
    "                config_name = emb_col.split(\"/\")[-1].replace(\"_EMB\", \"\")\n",
    "\n",
    "                model = MODEL_DICT[model_name]\n",
    "\n",
    "                retrieved_documents_list = retrieve_documents(\n",
    "                    query=query,\n",
    "                    model=model,\n",
    "                    documents=documents,\n",
    "                    df=df_recipes_cleaned,\n",
    "                    columns_to_select=DATA_COLUMNS,\n",
    "                    top_k=k\n",
    "                )\n",
    "\n",
    "                retrived_model_query_docs_dict \\\n",
    "                    .setdefault(k, {}) \\\n",
    "                    .setdefault(model_name, {}) \\\n",
    "                    .setdefault(config_name, {})[query] = retrieved_documents_list\n",
    "\n",
    "    # Save to file for future runs\n",
    "    with open(OUTPUT_RETRIEVED_DOCS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(retrived_model_query_docs_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7015ebb",
   "metadata": {},
   "source": [
    "**For each query get the unique doc retrived**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a613883",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_retrived_docs_dict = {}\n",
    "seen = {}\n",
    "\n",
    "for k_dict in retrived_model_query_docs_dict.values():\n",
    "    for model_dict in k_dict.values():\n",
    "        for config_dict in model_dict.values():\n",
    "            for query, docs in config_dict.items():\n",
    "                for doc in docs:\n",
    "                    clean_doc = {col: doc[col] for col in DATA_COLUMNS}\n",
    "                    doc_id = clean_doc['ID']\n",
    "                    \n",
    "                    if doc_id in seen.get(query, set()):\n",
    "                        continue\n",
    "\n",
    "                    seen.setdefault(query, set()).add(doc_id)\n",
    "                    \n",
    "                    query_retrived_docs_dict.setdefault(query, []).append(clean_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_retrived_docs_dict['15-minute high-protein vegan lunch for office meal prep no microwave'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_retrived_docs_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f0cfe",
   "metadata": {},
   "source": [
    "**LLM as judge to evaluate those retrived doc for each query**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c52fd",
   "metadata": {},
   "source": [
    "**load the prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_EVAL_PATH = config[\"eval_prompt_without_justification_file\"]\n",
    "\n",
    "with open(PROMPT_EVAL_PATH, \"r\") as f:\n",
    "    prompt_template = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92527ef8",
   "metadata": {},
   "source": [
    "**load the LLM and its config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9cfae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = config[\"llm_model\"]\n",
    "LLM_TEMPERATURE = config[\"temperature\"]\n",
    "LLM_MAX_TOKENS = config['max_tokens']\n",
    "LLM_MODEL_CONTEXT_WINDOWS = config[\"context_window\"] - LLM_MAX_TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719cf4e",
   "metadata": {},
   "source": [
    "**calculate the number of tokens in the prompt template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2438bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_token_prompt_template = len(str(prompt_template)) // 3\n",
    "num_token_prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f6360",
   "metadata": {},
   "source": [
    "**define the schema of the response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e1f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON schema for recipe relevance ratings\n",
    "json_schema = config['llm_json_format_without_justification']\n",
    "json_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbe3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRIES_PER_BATCH = 3\n",
    "\n",
    "llm_results = []\n",
    "\n",
    "# OUTPUT_DATA_DIR = config[\"output_data_dir\"].format(experiment_id=EXPERIENCE_ID)\n",
    "# os.makedirs(OUTPUT_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# OUTPUT_RETRIEVED_DOCS_RELEVENCE_FILE = config[\"output_retrived_documents_relevence_file\"].format(\n",
    "#     experiment_id=EXPERIENCE_ID    \n",
    "# )\n",
    "\n",
    "# # Check if the file exists\n",
    "# if os.path.exists(OUTPUT_RETRIEVED_DOCS_RELEVENCE_FILE):\n",
    "#     print(f\"Loading cached results from {OUTPUT_RETRIEVED_DOCS_RELEVENCE_FILE} ...\")\n",
    "#     with open(OUTPUT_RETRIEVED_DOCS_RELEVENCE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "#         llm_results = json.load(f)\n",
    "# else:\n",
    "# print(\"File not found. Running llm as judge calculations ...\")\n",
    "\n",
    "for query in query_retrived_docs_dict:\n",
    "    docs = query_retrived_docs_dict[query]\n",
    "    \n",
    "    query_doc_relevance_dict = {\n",
    "        \"query_text\": query,\n",
    "        \"relevance_judgments\": []\n",
    "    }\n",
    "    \n",
    "    \n",
    "    num_token_query = len(str(query)) // 3\n",
    "    num_token_docs = len(str(docs)) // 3\n",
    "    \n",
    "    doc_entries_list = split_docs_recursively(client=SnowflakeClient(), model=LLM_MODEL, max_tokens=LLM_MODEL_CONTEXT_WINDOWS, base_template_size=num_token_prompt_template, num_token_query=num_token_query, doc_entries=docs)\n",
    "    \n",
    "    for doc_batch in doc_entries_list:\n",
    "\n",
    "        remaining_docs = doc_batch[:]  # docs still needing scores\n",
    "        collected_scores = {}           # doc_id -> relevance_score\n",
    "        attempt = 0\n",
    "\n",
    "        while remaining_docs and attempt < MAX_RETRIES_PER_BATCH:\n",
    "            attempt += 1\n",
    "\n",
    "            prompt = build_prompt(prompt_template, query, remaining_docs)\n",
    "\n",
    "            try:\n",
    "                llm_response = get_llm_response(\n",
    "                    client=SnowflakeClient(),\n",
    "                    model=LLM_MODEL,\n",
    "                    prompt=prompt,\n",
    "                    response_format=json_schema,\n",
    "                    temperature=LLM_TEMPERATURE,\n",
    "                    max_tokens=LLM_MAX_TOKENS,\n",
    "                )\n",
    "\n",
    "                json_output = json.loads(llm_response)\n",
    "\n",
    "                returned_docs = {\n",
    "                    d[\"doc_id\"]: d[\"relevance_score\"]\n",
    "                    for d in json_output[\"relevance_judgments\"]\n",
    "                }\n",
    "\n",
    "                # Merge newly returned scores\n",
    "                collected_scores.update(returned_docs)\n",
    "\n",
    "                # Detect still-missing docs\n",
    "                returned_ids = set(returned_docs.keys())\n",
    "                remaining_docs = [\n",
    "                    doc for doc in remaining_docs if doc[\"ID\"] not in returned_ids\n",
    "                ]\n",
    "\n",
    "                if remaining_docs:\n",
    "                    print(\n",
    "                        f\"Retry {attempt}/{MAX_RETRIES_PER_BATCH} | \"\n",
    "                        f\"Missing {len(remaining_docs)} docs for query='{query}'\"\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"âŒ LLM error on retry {attempt} \"\n",
    "                    f\"(batch size={len(remaining_docs)})\"\n",
    "                )\n",
    "                print(e)\n",
    "                break\n",
    "\n",
    "        # ðŸ§© Finalize batch (guaranteed coverage)\n",
    "        for doc in doc_batch:\n",
    "            doc_id = doc[\"ID\"]\n",
    "            query_doc_relevance_dict[\"relevance_judgments\"].append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"relevance_score\": collected_scores.get(doc_id, 0.0)\n",
    "            })\n",
    "\n",
    "        # Log hard failures (after retries)\n",
    "        if remaining_docs:\n",
    "            print(\n",
    "                f\"FINAL MISSING after {MAX_RETRIES_PER_BATCH} retries \"\n",
    "                f\"for query='{query}': {[d['ID'] for d in remaining_docs]}\"\n",
    "            )\n",
    "  \n",
    "    llm_results.append(query_doc_relevance_dict)    \n",
    "    break  \n",
    "\n",
    "    print(f\"âœ“ Parsed JSON for query: {query}\")\n",
    "\n",
    "# #Save to file for future runs\n",
    "# with open(OUTPUT_RETRIEVED_DOCS_RELEVENCE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(llm_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c6483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43481055",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_results.append(query_doc_relevance_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ef046",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_RETRIEVED_DOCS_RELEVENCE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        llm_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf656029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "merged_llm_results = {}\n",
    "\n",
    "for item in llm_results:\n",
    "    query = item[\"query_text\"]\n",
    "    judgments = item.get(\"relevance_judgments\", [])\n",
    "\n",
    "    if query not in merged_llm_results:\n",
    "        merged_llm_results[query] = {\n",
    "            \"query_text\": query,\n",
    "            \"relevance_judgments\": []\n",
    "        }\n",
    "\n",
    "    merged_llm_results[query][\"relevance_judgments\"].extend(judgments)\n",
    "\n",
    "# Convert back to list (same format as before)\n",
    "llm_results_merged = list(merged_llm_results.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cbf4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_retrived_docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the doc with id 184442 and query a vegan dessert that require minimum time to prepare\n",
    "[doc for doc in llm_results_merged[0][\"relevance_judgments\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e1c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_retrived_docs_dict['a vegan dessert that require minimum time to prepare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b99cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[doc for doc in query_retrived_docs_dict['a vegan dessert that require minimum time to prepare'] if doc['ID'] == 184442]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_id_initial = set([doc['ID'] for doc in query_retrived_docs_dict['a vegan dessert that require minimum time to prepare']])\n",
    "doc_id_llm = set([doc['doc_id'] for doc in llm_results_merged[0][\"relevance_judgments\"]])\n",
    "\n",
    "doc_id_initial - doc_id_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97764248",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = set([doc['ID'] for doc in doc_entries_list[0]])\n",
    "second_batch = set([doc['ID'] for doc in doc_entries_list[1]])\n",
    "entire_bath = first_batch.union(second_batch)\n",
    "doc_id_initial - entire_bath "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_entries_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bf950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([doc['ID'] for doc in query_retrived_docs_dict['easy baking']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([doc['doc_id'] for doc in llm_results_merged[34][\"relevance_judgments\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea2c590",
   "metadata": {},
   "source": [
    "**Associate the relevancy score to each pair of (query, doc) for each top-k, model, config** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccecbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build lookup: relevance_map[query][doc_id] = relevance_score\n",
    "relevance_map = {}\n",
    "\n",
    "for item in llm_results_merged:\n",
    "    query = item[\"query_text\"]\n",
    "    relevance_map[query] = {\n",
    "        r[\"doc_id\"]: r[\"relevance_score\"]\n",
    "        for r in item[\"relevance_judgments\"]\n",
    "    }\n",
    "\n",
    "for k, model_dict in retrived_model_query_docs_dict.items():\n",
    "    for model_name, config_dict in model_dict.items():\n",
    "        for config_name, query_dict in config_dict.items():\n",
    "            for query, docs in query_dict.items():\n",
    "\n",
    "                # skip if query was not judged by LLM\n",
    "                if query not in relevance_map:\n",
    "                    continue\n",
    "\n",
    "                for doc in docs:\n",
    "                    doc_id = doc.get(\"ID\")\n",
    "\n",
    "                    # attach relevance score if available\n",
    "                    doc[\"relevance_score\"] = relevance_map[query].get(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832c29d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrived_model_query_docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrived_model_query_docs_dict[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
