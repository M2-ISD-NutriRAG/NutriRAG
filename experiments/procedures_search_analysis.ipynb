{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "6djxiwivvypep44mlur4",
   "authorId": "3571954838300",
   "authorName": "BLUEJAY",
   "authorEmail": "",
   "sessionId": "56295028-6669-4bda-a941-19f5a750443a",
   "lastEditTime": 1767291523469
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\n\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8efe0fc6-5d29-42f8-b990-318b45bde4ad",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import snowflake.snowpark.functions as F\nfrom snowflake.snowpark.types import StringType, StructType, StructField\nimport pandas as pd\nimport json\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# === CONFIGURATION ===\n# Table & Index Names (Based on your docs)\nSOURCE_TABLE = \"ENRICHED.RECIPES_SAMPLE_50K\"\nBM25_INDEX = \"VECTORS.RECIPES_SAMPLE_50K_BM25_INDEX\"\nEMBEDDINGS_TABLE = \"VECTORS.RECIPES_50K_EMBEDDINGS\"\n\n# Procedure Names\nPROC_BM25 = \"NUTRIRAG_PROJECT.VECTORS.SEARCH_BM25\"\nPROC_SEMANTIC = \"NUTRIRAG_PROJECT.VECTORS.SEARCH_SEMANTIC\"\nPROC_HYBRID = \"NUTRIRAG_PROJECT.VECTORS.SEARCH_SIMILAR_RECIPES\"\n\n# Hybrid Weights\nVECTOR_WEIGHT = 0.7\nBM25_WEIGHT = 0.3\n\n# Cortex Model for Evaluation (LLM Judge)\nJUDGE_MODEL = \"llama3.1-70b\" # or 'mistral-large' <- trés couteux ce truc ptn\n\n# Test Parameters\nTOP_K = 5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb3715ff-9e3a-4832-ae66-12f2aeba5930",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "test_queries = [\n    \"chocolate cake\",                     # Keyword heavy\n    \"healthy vegetarian dinner quick\",    # Semantic + Constraint\n    \"gluten free pasta\",   \n    \"refreshing summer dessert lemon\",    # Semantic / Vibe\n    \"traditional italian lasagna\",        # Specific entity\n    \"high protein breakfast\",     \n    \"comfort food for rainy day\",         # Purely semantic/abstract\n    \"romantic dinner for two italian style\",  # Vibe + Cuisine\n    \"something easy to digest when sick\",     # Fonctionnel (BM25 va chercher le mot \"sick\"...)\n    \"meal to impress my boss\",                # Social / Abstrait\n    \"post workout high protein recovery\",     # Fonctionnel / Nutrition\n    \"cozy comfort food for winter night\",     # Vibe / Saison\n    \"light lunch that won't make me sleepy\",  # Effet désiré\n    \"kids friendly vegetables dish\"           # Cible\n]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b784874-ed34-4969-bc1d-497e50bd0e59",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def run_bm25(query):\n    \"\"\"Executes BM25 Search Procedure\"\"\"\n    try:\n        # Args: Query, IndexTable, SourceTable, Limit, Filters\n        # Note: Filters are NULL (None) for this benchmark\n        res = session.call(PROC_BM25, query, BM25_INDEX, SOURCE_TABLE, TOP_K, None)\n        return json.loads(res) if isinstance(res, str) else res\n    except Exception as e:\n        print(f\"Error in BM25: {e}\")\n        return []\n\ndef run_semantic(query):\n    \"\"\"Executes Semantic Search Procedure\"\"\"\n    try:\n        # Args: Query, EmbeddingsTable, Limit, Filters\n        res = session.call(PROC_SEMANTIC, query, EMBEDDINGS_TABLE, TOP_K, None)\n        return json.loads(res) if isinstance(res, str) else res\n    except Exception as e:\n        print(f\"Error in Semantic: {e}\")\n        return []\n\ndef run_hybrid(query):\n    \"\"\"Executes Hybrid Search Procedure\"\"\"\n    try:\n        # Args: Query, Limit, Filters, VecWeight, BM25Weight, Index, Source, Embeddings\n        res = session.call(\n            PROC_HYBRID, \n            query, \n            TOP_K, \n            None, \n            VECTOR_WEIGHT, \n            BM25_WEIGHT, \n            BM25_INDEX, \n            SOURCE_TABLE, \n            EMBEDDINGS_TABLE\n        )\n        return json.loads(res) if isinstance(res, str) else res\n    except Exception as e:\n        print(f\"Error in Hybrid: {e}\")\n        return []",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c90910f1-dbeb-4115-a14f-610e915f39e6",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "results_data = []\n\nprint(f\"Starting benchmark for {len(test_queries)} queries...\")\n\nfor q in test_queries:\n    # 1. BM25\n    start = time.time()\n    res_bm25 = run_bm25(q)\n    dur_bm25 = (time.time() - start) * 1000 # ms\n    \n    # 2. Semantic\n    start = time.time()\n    res_sem = run_semantic(q)\n    dur_sem = (time.time() - start) * 1000 # ms\n    \n    # 3. Hybrid\n    start = time.time()\n    res_hyb = run_hybrid(q)\n    dur_hyb = (time.time() - start) * 1000 # ms\n    \n    # Store raw results for the LLM Judge\n    results_data.append({\n        \"query\": q,\n        \"bm25_res\": res_bm25,\n        \"bm25_time\": dur_bm25,\n        \"sem_res\": res_sem,\n        \"sem_time\": dur_sem,\n        \"hyb_res\": res_hyb,\n        \"hyb_time\": dur_hyb\n    })\n\nprint(\"Benchmark execution complete.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aae69195-a747-4063-b558-92860347011e",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "for i, row in enumerate(results_data):\n    print(f\"\\n==================================================\")\n    print(f\"QUERY {i+1}: '{row['query']}'\")\n    print(f\"==================================================\")\n    \n    # Extract just the top 3 recipe names for clarity\n    bm25_names = [r.get('NAME', 'Unknown') for r in row['bm25_res'][:3]]\n    sem_names = [r.get('NAME', 'Unknown') for r in row['sem_res'][:3]]\n    hyb_names = [r.get('NAME', 'Unknown') for r in row['hyb_res'][:3]]\n    \n    # Print comparison\n    print(f\"--- BM25 Found: ---\")\n    for name in bm25_names: print(f\"  • {name}\")\n        \n    print(f\"\\n--- SEMANTIC Found: ---\")\n    for name in sem_names: print(f\"  • {name}\")\n        \n    print(f\"\\n--- HYBRID Found: ---\")\n    for name in hyb_names: print(f\"  • {name}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4e45423-eb89-40b6-9cbb-66789a8499a1",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "\nPROMPT_TEMPLATE = \"\"\"\nTu es un expert en recherche d’information (Information Retrieval) et en évaluation de moteurs de recherche.\nTon objectif est d’évaluer objectivement la qualité des résultats fournis par trois systèmes différents pour une même requête utilisateur.\n\n=== REQUÊTE UTILISATEUR ===\n{query_text}\n\n=== RÉSULTATS DU SYSTÈME A (BM25) ===\n{bm25_docs}\n\n=== RÉSULTATS DU SYSTÈME B (SEMANTIQUE) ===\n{sem_docs}\n\n=== RÉSULTATS DU SYSTÈME C (HYBRIDE) ===\n{hyb_docs}\n\n=== CRITÈRES D’ÉVALUATION ===\n1. Pertinence globale\n2. Couverture de l’intention\n3. Précision du top 3\n4. Absence de résultats hors sujet\n\n=== FORMAT DE SORTIE OBLIGATOIRE (JSON STRICT) ===\nRetourne uniquement un JSON valide, sans texte supplémentaire :\n{{\n  \"BM25\": {{ \"score\": <float 0-5>, \"justification\": \"...\" }},\n  \"SEMANTIQUE\": {{ \"score\": <float 0-5>, \"justification\": \"...\" }},\n  \"HYBRIDE\": {{ \"score\": <float 0-5>, \"justification\": \"...\" }},\n  \"winner\": \"BM25 | SEMANTIQUE | HYBRIDE | TIE\"\n}}\n\"\"\"\n\ndef format_docs(results):\n    \"\"\"Helper to format list of recipes into string for LLM\"\"\"\n    if not results: return \"Aucun résultat.\"\n    return \"\\n\".join([f\"{i+1}. {r.get('NAME', 'Unknown')} (Desc: {r.get('DESCRIPTION', '')[:100]}...)\" for i, r in enumerate(results)])\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "370ca90a-b358-4334-abac-b39f159e11dc",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.cortex import Complete\n\neval_metrics = []\n\nprint(\"Running Cortex Evaluation (LLM-as-a-Judge)...\")\n\nfor row in results_data:\n    # Prepare Prompt\n    prompt = PROMPT_TEMPLATE.format(\n        query_text=row['query'],\n        bm25_docs=format_docs(row['bm25_res']),\n        sem_docs=format_docs(row['sem_res']),\n        hyb_docs=format_docs(row['hyb_res'])\n    )\n    \n    # Call Cortex\n    try:\n        response_str = Complete(JUDGE_MODEL, prompt)\n        # Simple cleanup in case LLM adds markdown blocks\n        clean_json = response_str.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n        eval_json = json.loads(clean_json)\n        \n        eval_metrics.append({\n            \"query\": row['query'],\n            \"bm25_score\": eval_json['BM25']['score'],\n            \"sem_score\": eval_json['SEMANTIQUE']['score'],\n            \"hyb_score\": eval_json['HYBRIDE']['score'],\n            \"winner\": eval_json['winner']\n        })\n    except Exception as e:\n        print(f\"Error evaluating query '{row['query']}': {e}\")\n\n# Create DataFrame\ndf_eval = pd.DataFrame(eval_metrics)\ndf_time = pd.DataFrame([{\n    \"query\": r['query'], \n    \"bm25_time\": r['bm25_time'], \n    \"sem_time\": r['sem_time'], \n    \"hyb_time\": r['hyb_time']\n} for r in results_data])\n\n# Merge\nfinal_df = pd.merge(df_time, df_eval, on=\"query\")\nfinal_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8a3581c-38ea-4bb2-a760-ec9718811a76",
   "metadata": {
    "language": "python",
    "name": "cell8",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Reshape for plotting\ndf_melted_time = final_df.melt(id_vars=[\"query\"], value_vars=[\"bm25_time\", \"sem_time\", \"hyb_time\"], \n                               var_name=\"Method\", value_name=\"Time (ms)\")\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=df_melted_time, x=\"query\", y=\"Time (ms)\", hue=\"Method\")\nplt.xticks(rotation=45, ha='right')\nplt.title(\"Execution Time by Method\")\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ba3a66b4-b4d9-4623-9bfb-3385860426fc",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Reshape for plotting\ndf_melted_score = final_df.melt(id_vars=[\"query\"], value_vars=[\"bm25_score\", \"sem_score\", \"hyb_score\"], \n                                var_name=\"Method\", value_name=\"Quality Score (0-5)\")\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=df_melted_score, x=\"query\", y=\"Quality Score (0-5)\", hue=\"Method\", palette=\"viridis\")\nplt.xticks(rotation=45, ha='right')\nplt.title(\"LLM Quality Evaluation Scores\")\nplt.ylim(0, 5.5)\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "774287d1-b361-4abd-948f-4ba415e6bb6f",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "# --- 1. Strategic Trade-off Scatter Plot ---\nplt.figure(figsize=(8, 6))\n\n# Calculate Averages\navg_data = final_df.mean(numeric_only=True)\nmethods = [('BM25', 'bm25'), ('Semantic', 'sem'), ('Hybrid', 'hyb')]\n\n# Plot points\nfor label, key in methods:\n    score = avg_data[f'{key}_score']\n    time_val = avg_data[f'{key}_time']\n    plt.scatter(time_val, score, s=300, label=label)\n    # Label with Score\n    plt.text(time_val+1000, score, \n             f\"  {label}\\n  (Score: {score:.2f})\", \n             va='center', fontweight='bold')\n\n# Draw arrows to show the \"Upgrade Path\"\nplt.plot([avg_data['bm25_time'], avg_data['sem_time'], avg_data['hyb_time']], \n         [avg_data['bm25_score'], avg_data['sem_score'], avg_data['hyb_score']], \n         '--', color='gray', alpha=0.5)\n\nplt.title(\"Strategic Trade-off: Latency vs. Quality\")\nplt.xlabel(\"Latency (ms)\")\nplt.ylabel(\"Quality Score (0-5)\")\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n# --- 2. Net Improvement Calculation ---\nfinal_df['delta_vs_sem'] = final_df['hyb_score'] - final_df['sem_score']\nfinal_df['delta_vs_bm25'] = final_df['hyb_score'] - final_df['bm25_score']\n\nprint(\"\\n=== NET IMPROVEMENT ANALYSIS ===\")\nprint(f\"Average Improvement (Hybrid vs Semantic): +{final_df['delta_vs_sem'].mean():.2f} points\")\nprint(f\"Average Improvement (Hybrid vs BM25):     +{final_df['delta_vs_bm25'].mean():.2f} points\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d4444cd0-c79d-45ec-ac3f-ab38a708f8fa",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "summary = final_df.agg({\n    'bm25_time': 'mean',\n    'sem_time': 'mean',\n    'hyb_time': 'mean',\n    'bm25_score': 'mean',\n    'sem_score': 'mean',\n    'hyb_score': 'mean'\n}).transpose()\n\nprint(\"=== AVERAGE METRICS ===\")\nprint(summary)\n\nprint(\"\\n=== WINNER DISTRIBUTION ===\")\nprint(final_df['winner'].value_counts())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "477a352c-d2b7-4beb-8140-05cca3476187",
   "metadata": {
    "language": "python",
    "name": "cell15",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def format_docs(results):\n    \"\"\"\n    Transforme la liste JSON des recettes en un texte lisible pour le LLM.\n    Gère le cas où la liste est vide.\n    \"\"\"\n    if not results or len(results) == 0:\n        return \"Aucun résultat trouvé.\"\n    \n    formatted_text = \"\"\n    for i, r in enumerate(results[:5]): # On limite aux 5 premiers pour ne pas saturer le contexte\n        name = r.get('NAME', 'Recette Inconnue')\n        # On coupe la description pour économiser des tokens\n        desc = r.get('DESCRIPTION', 'Pas de description')[:150].replace(\"\\n\", \" \")\n        formatted_text += f\"{i+1}. {name} (Desc: {desc}...)\\n\"\n    \n    return formatted_text\n\nPROMPT_TEMPLATE = \"\"\"\nTu es un expert impartial en évaluation de moteurs de recherche (Information Retrieval).\nTa mission est de noter la qualité des résultats pour une requête culinaire donnée.\n\n=== REQUÊTE UTILISATEUR ===\n\"{query_text}\"\n\n=== RÉSULTATS À ÉVALUER (HYBRIDE) ===\n{hyb_docs}\n\n=== CRITÈRES DE NOTATION (0 à 5) ===\n- 5.0 : Parfait. Résultats pertinents, respectent toutes les contraintes (ingrédients, régime).\n- 4.0 : Très bon. Pertinent mais ordre perfectible.\n- 3.0 : Acceptable. Quelques résultats pertinents, d'autres moins.\n- 1.0 - 2.0 : Mauvais. Hors sujet ou non respect des contraintes critiques (ex: allergènes).\n- 0.0 : Aucun résultat ou résultats vides.\n\n=== FORMAT DE SORTIE OBLIGATOIRE ===\nTu dois répondre UNIQUEMENT avec un JSON valide.\nNe mets PAS de balises Markdown (pas de ```json).\nNe mets PAS de phrase d'introduction.\n\nStructure attendue :\n{{\n  \"HYBRIDE\": {{\n    \"score\": <nombre flottant entre 0 et 5>,\n    \"justification\": \"<courte explication en 1 phrase>\"\n  }}\n}}\n\"\"\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9a1b49e-ba67-41c2-9fb0-2667913fab45",
   "metadata": {
    "language": "python",
    "name": "cell12",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "weight_candidates = [\n    (0.3, 0.7),\n    (0.4, 0.6),\n    (0.6, 0.4),\n    (0.7, 0.3),\n    (0.8, 0.2),\n]\n\nfilters_dict = {\n  \"numeric_filters\": [\n        {\"name\": \"minutes\", \"operator\": \"<=\", \"value\": 50},\n        {\"name\": \"servings\", \"operator\": \">=\", \"value\": 2}\n    ]\n}\nfilters = json.dumps(filters_dict)\n\ntuning_results = []\n\nprint(f\"=== DÉMARRAGE ({len(weight_candidates)} configs) ===\\n\")\n\nfor vec_w, bm25_w in weight_candidates:\n    print(f\"\\n>> CONFIG: Vector={vec_w} / BM25={bm25_w}\")\n    current_scores = []\n    \n    for q in test_queries:\n        try:\n            res_hyb = session.call(\n                PROC_HYBRID, q, TOP_K, filters, vec_w, bm25_w, \n                BM25_INDEX, SOURCE_TABLE, EMBEDDINGS_TABLE\n            )\n            \n            if not res_hyb:\n                print(f\"   [WARN] Recherche vide pour '{q}'\")\n                continue\n\n            try:\n                res_json = json.loads(res_hyb) if isinstance(res_hyb, str) else res_hyb\n            except json.JSONDecodeError:\n                print(f\"   [ERREUR SP] La procédure n'a pas renvoyé du JSON pour '{q}'.\")\n                print(f\"   [CONTENU REÇU] : {str(res_hyb)[:100]}...\") # Affiche le début pour comprendre\n                continue\n                \n        except Exception as e:\n            print(f\"   [CRASH SP] Erreur d'appel procédure sur '{q}': {e}\")\n            continue\n\n        try:\n            # On simplifie le prompt pour éviter de perdre le LLM\n            prompt = PROMPT_TEMPLATE.format(\n                query_text=q, \n                bm25_docs=\"[]\", \n                sem_docs=\"[]\", \n                hyb_docs=format_docs(res_json)\n            )\n            \n            cmd = \"SELECT snowflake.cortex.COMPLETE(?, ?)\"\n            cortex_res = session.sql(cmd, params=[JUDGE_MODEL, prompt]).collect()[0][0]\n            \n            if not cortex_res:\n                print(f\"   [WARN] Cortex a renvoyé une réponse vide pour '{q}'\")\n                continue\n\n            # Nettoyage et Parsing JSON Cortex\n            clean_json = cortex_res.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n            \n            try:\n                eval_data = json.loads(clean_json)\n                score = eval_data['HYBRIDE']['score']\n                justif = eval_data['HYBRIDE']['justification']\n\n                # AFFICHAGE DÉTAILLÉ\n                print(f\"   Query: {q}\")\n                print(f\"   Note : {score}/5\")\n                print(f\"   Avis : {justif}\")\n                print(\"   ---\")\n            \n                current_scores.append(score)\n            except json.JSONDecodeError:\n                print(f\"   [ERREUR LLM] Cortex n'a pas renvoyé du JSON valide pour '{q}'.\")\n                print(f\"   [CONTENU REÇU] : {clean_json[:100]}...\")\n                continue\n                \n        except Exception as e:\n            print(f\"   [CRASH LLM] Erreur Cortex sur '{q}': {e}\")\n            continue\n\n    # Calcul Moyenne\n    if current_scores:\n        avg = sum(current_scores) / len(current_scores)\n        tuning_results.append({\"vector_weight\": vec_w, \"avg_score\": avg})\n        print(f\"   -> Score Moyen: {avg:.2f}/5.0\")\n    else:\n        print(\"   -> Config ignorée (pas assez de données valides).\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8dedeb0a-d1a2-437e-b6df-adc1d6373932",
   "metadata": {
    "language": "python",
    "name": "cell13",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Visualisation des Résultats (Courbe de Sensibilité)\ndf_tuning = pd.DataFrame(tuning_results)\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=df_tuning, x=\"vector_weight\", y=\"avg_score\", marker=\"o\", markersize=10, linewidth=2.5, color=\"#2ecc71\")\n\nplt.title(\"Impact du Poids Vectoriel sur la Qualité (Sensitivity Analysis)\")\nplt.xlabel(\"Poids Vectoriel (0.0 = BM25 pur, 1.0 = Sémantique pur)\")\nplt.ylabel(\"Score Moyen de Qualité (0-5)\")\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.ylim(2.5, 5.2) # Ajuster selon vos scores min/max\n\n# Annoter le meilleur point\nbest_config = df_tuning.loc[df_tuning['avg_score'].idxmax()]\nplt.annotate(f\"Meilleur: {best_config['avg_score']:.2f}\\n(Vec: {best_config['vector_weight']})\", \n             xy=(best_config['vector_weight'], best_config['avg_score']), \n             xytext=(0, 40), textcoords='offset points', ha='center',\n             arrowprops=dict(facecolor='black', shrink=0.05))\n\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eb7b906e-d22b-4550-8858-320c794988d0",
   "metadata": {
    "language": "python",
    "name": "cell17",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}