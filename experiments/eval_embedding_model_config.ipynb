{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "966e48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db299b4",
   "metadata": {},
   "source": [
    "**laod the config file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "62254183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "CONFIG_FILE_PATH = os.getenv(\"CONFIG_FILE_PATH\")\n",
    "\n",
    "with open(CONFIG_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7a3300",
   "metadata": {},
   "source": [
    "**set the experience id** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "82719237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "EXPERIENCE_ID = config[\"experiments_specifique_params\"][\"experiment_id\"]\n",
    "\n",
    "print(EXPERIENCE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdab1de1",
   "metadata": {},
   "source": [
    "**load the embedding file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "0daaa4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embedding columns: ['intfloat/e5-base-v2/config_1_EMB', 'sentence-transformers/all-MiniLM-L6-v2/config_1_EMB', 'BAAI/bge-base-en-v1.5/config_1_EMB', 'Snowflake/snowflake-arctic-embed-m/config_1_EMB', 'Snowflake/snowflake-arctic-embed-m-v1.5/config_1_EMB', 'intfloat/e5-base-v2/config_2_EMB', 'sentence-transformers/all-MiniLM-L6-v2/config_2_EMB', 'BAAI/bge-base-en-v1.5/config_2_EMB', 'Snowflake/snowflake-arctic-embed-m/config_2_EMB', 'Snowflake/snowflake-arctic-embed-m-v1.5/config_2_EMB', 'intfloat/e5-base-v2/config_3_EMB', 'sentence-transformers/all-MiniLM-L6-v2/config_3_EMB', 'BAAI/bge-base-en-v1.5/config_3_EMB', 'Snowflake/snowflake-arctic-embed-m/config_3_EMB', 'Snowflake/snowflake-arctic-embed-m-v1.5/config_3_EMB', 'intfloat/e5-base-v2/config_4_EMB', 'sentence-transformers/all-MiniLM-L6-v2/config_4_EMB', 'BAAI/bge-base-en-v1.5/config_4_EMB', 'Snowflake/snowflake-arctic-embed-m/config_4_EMB', 'Snowflake/snowflake-arctic-embed-m-v1.5/config_4_EMB', 'intfloat/e5-base-v2/config_5_EMB', 'sentence-transformers/all-MiniLM-L6-v2/config_5_EMB', 'BAAI/bge-base-en-v1.5/config_5_EMB', 'Snowflake/snowflake-arctic-embed-m/config_5_EMB', 'Snowflake/snowflake-arctic-embed-m-v1.5/config_5_EMB', 'intfloat/e5-base-v2/config_6_EMB', 'sentence-transformers/all-MiniLM-L6-v2/config_6_EMB', 'BAAI/bge-base-en-v1.5/config_6_EMB', 'Snowflake/snowflake-arctic-embed-m/config_6_EMB', 'Snowflake/snowflake-arctic-embed-m-v1.5/config_6_EMB']\n",
      "intfloat/e5-base-v2/config_1_EMB -> first embedding shape: (768,)\n",
      "sentence-transformers/all-MiniLM-L6-v2/config_1_EMB -> first embedding shape: (384,)\n",
      "BAAI/bge-base-en-v1.5/config_1_EMB -> first embedding shape: (768,)\n",
      "Snowflake/snowflake-arctic-embed-m/config_1_EMB -> first embedding shape: (768,)\n",
      "Snowflake/snowflake-arctic-embed-m-v1.5/config_1_EMB -> first embedding shape: (768,)\n",
      "intfloat/e5-base-v2/config_2_EMB -> first embedding shape: (768,)\n",
      "sentence-transformers/all-MiniLM-L6-v2/config_2_EMB -> first embedding shape: (384,)\n",
      "BAAI/bge-base-en-v1.5/config_2_EMB -> first embedding shape: (768,)\n",
      "Snowflake/snowflake-arctic-embed-m/config_2_EMB -> first embedding shape: (768,)\n",
      "Snowflake/snowflake-arctic-embed-m-v1.5/config_2_EMB -> first embedding shape: (768,)\n",
      "intfloat/e5-base-v2/config_3_EMB -> first embedding shape: (768,)\n",
      "sentence-transformers/all-MiniLM-L6-v2/config_3_EMB -> first embedding shape: (384,)\n",
      "BAAI/bge-base-en-v1.5/config_3_EMB -> first embedding shape: (768,)\n",
      "Snowflake/snowflake-arctic-embed-m/config_3_EMB -> first embedding shape: (768,)\n",
      "Snowflake/snowflake-arctic-embed-m-v1.5/config_3_EMB -> first embedding shape: (768,)\n",
      "intfloat/e5-base-v2/config_4_EMB -> first embedding shape: (768,)\n",
      "sentence-transformers/all-MiniLM-L6-v2/config_4_EMB -> first embedding shape: (384,)\n",
      "BAAI/bge-base-en-v1.5/config_4_EMB -> first embedding shape: (768,)\n",
      "Snowflake/snowflake-arctic-embed-m/config_4_EMB -> first embedding shape: (768,)\n",
      "Snowflake/snowflake-arctic-embed-m-v1.5/config_4_EMB -> first embedding shape: (768,)\n",
      "intfloat/e5-base-v2/config_5_EMB -> first embedding shape: (768,)\n",
      "sentence-transformers/all-MiniLM-L6-v2/config_5_EMB -> first embedding shape: (384,)\n",
      "BAAI/bge-base-en-v1.5/config_5_EMB -> first embedding shape: (768,)\n",
      "Snowflake/snowflake-arctic-embed-m/config_5_EMB -> first embedding shape: (768,)\n",
      "Snowflake/snowflake-arctic-embed-m-v1.5/config_5_EMB -> first embedding shape: (768,)\n",
      "intfloat/e5-base-v2/config_6_EMB -> first embedding shape: (768,)\n",
      "sentence-transformers/all-MiniLM-L6-v2/config_6_EMB -> first embedding shape: (384,)\n",
      "BAAI/bge-base-en-v1.5/config_6_EMB -> first embedding shape: (768,)\n",
      "Snowflake/snowflake-arctic-embed-m/config_6_EMB -> first embedding shape: (768,)\n",
      "Snowflake/snowflake-arctic-embed-m-v1.5/config_6_EMB -> first embedding shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "INPUT_EMBEDDINGS_FILE = config[\"output_recipies_embedding_file\"].format(\n",
    "    experiment_id=EXPERIENCE_ID\n",
    ")\n",
    "df_recipes_cleaned = pd.read_csv(INPUT_EMBEDDINGS_FILE)\n",
    "\n",
    "\n",
    "emb_columns = [col for col in df_recipes_cleaned.columns if col.endswith('_EMB')]\n",
    "print(f\"Found embedding columns: {emb_columns}\")\n",
    "\n",
    "for col in emb_columns:\n",
    "    df_recipes_cleaned[col] = df_recipes_cleaned[col].apply(\n",
    "        lambda x: np.fromstring(x.strip('[]'), sep=' ', dtype=np.float32)\n",
    "    )\n",
    "\n",
    "for col in emb_columns:\n",
    "    print(f\"{col} -> first embedding shape: {df_recipes_cleaned[col][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d42f4",
   "metadata": {},
   "source": [
    "**load the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "5cb256fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "MODELS_CONFIG = config[\"models\"]\n",
    "COLUMNS_TO_EMBEDDE = config[\"columns_embedding\"]\n",
    "\n",
    "#create a dict {name model : model} \n",
    "MODELS_LIST = [SentenceTransformer(model_id) for model_id in MODELS_CONFIG]\n",
    "MODEL_DICT = dict(zip(MODELS_CONFIG, MODELS_LIST))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72b6a2",
   "metadata": {},
   "source": [
    "**get the ground truth for query retrival**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "c83ee2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/query/query_test.json\n",
      "[{'query_text': 'easy breakfast smoothie vegan protein-packed', 'documents': [105717, 195909, 251917, 343717, 288453, 39520, 126940, 124000]}]\n"
     ]
    }
   ],
   "source": [
    "#create query dict\n",
    "QUERY_FILE_PATH = config[\"query_file_path\"]\n",
    "\n",
    "with open(QUERY_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    query_documents_dicts = json.load(f)\n",
    "\n",
    "print(QUERY_FILE_PATH)\n",
    "print(query_documents_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44de73",
   "metadata": {},
   "source": [
    "**set retrival metric folder and file path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "83358eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiments/exp1/metrics\n",
      "experiments/exp1/metrics/retrival_metrics.json\n",
      "experiments/exp1/metrics/retrival_per_query.json\n"
     ]
    }
   ],
   "source": [
    "METRIC_FOLDER_PATH = config[\"output_retrival_dir\"].format(\n",
    "    experiment_id=EXPERIENCE_ID \n",
    ")\n",
    "\n",
    "#create the folder if not exists\n",
    "os.makedirs(METRIC_FOLDER_PATH, exist_ok=True)\n",
    "\n",
    "RETRIVED_DOCUMENTS_FILE = config[\"output_retrival_file\"].format(\n",
    "    experiment_id=EXPERIENCE_ID \n",
    ")\n",
    "\n",
    "METRIC_QUERY_FILE = config[\"output_query_metrics_file\"].format(\n",
    "    experiment_id=EXPERIENCE_ID\n",
    ")\n",
    "\n",
    "METRIC_PER_QUERY_FILE = config[\"output_per_query_file\"].format(\n",
    "    experiment_id=EXPERIENCE_ID \n",
    ")\n",
    "\n",
    "\n",
    "print(METRIC_FOLDER_PATH)\n",
    "print(METRIC_QUERY_FILE)\n",
    "print(METRIC_PER_QUERY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0908c",
   "metadata": {},
   "source": [
    "**define function to retrive documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "65574225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query: str, model: SentenceTransformer, documents: list, df: pd.DataFrame, top_k: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve top-k documents relevant to the query using the specified embedding model.\n",
    "    \"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True, device=device)\n",
    "    query_embedding = normalize(query_embedding, p=2, dim=1)\n",
    "\n",
    "    # Build document embedding tensor on the same device\n",
    "    document_embeddings = torch.stack(\n",
    "        [torch.tensor(doc, dtype=query_embedding.dtype, device=device) for doc in documents]\n",
    "    )\n",
    "    document_embeddings = normalize(document_embeddings, p=2, dim=1)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_scores = torch.matmul(query_embedding, document_embeddings.T).squeeze(0)\n",
    "\n",
    "    # Top-k\n",
    "    top_k_results = torch.topk(cosine_scores, k=top_k)\n",
    "    top_k_indices = top_k_results.indices.tolist()\n",
    "    top_k_scores = top_k_results.values.tolist()\n",
    "\n",
    "    retrieved_df = df.iloc[top_k_indices].copy()\n",
    "    retrieved_df[\"similarity_score\"] = top_k_scores\n",
    "\n",
    "    return retrieved_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07803597",
   "metadata": {},
   "source": [
    "**define functions that calculate metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "4cbc1b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute precison at K\n",
    "def compute_precision_at_k(retrieved_docs: list, relevant_docs: list, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute Precision at K for the retrieved documents.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs (list): List of retrieved document IDs.\n",
    "        relevant_docs (list): List of relevant document IDs.\n",
    "        k (int): The cutoff rank K.\n",
    "\n",
    "    Returns:\n",
    "        float: Precision at K value.\n",
    "    \"\"\"\n",
    "    # Get the top-k retrieved documents\n",
    "    top_k_retrieved = retrieved_docs[:k]\n",
    "    \n",
    "    # Count the number of relevant documents in the top-k retrieved documents\n",
    "    relevant_retrieved_count = sum(1 for doc_id in top_k_retrieved if doc_id in relevant_docs)\n",
    "    \n",
    "    # Calculate Precision at K\n",
    "    precision_at_k = relevant_retrieved_count / k\n",
    "    \n",
    "    return precision_at_k\n",
    "\n",
    "#compute recall at K\n",
    "def compute_recall_at_k(retrieved_docs: list, relevant_docs: list, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute Recall at K for the retrieved documents.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs (list): List of retrieved document IDs.\n",
    "        relevant_docs (list): List of relevant document IDs.\n",
    "        k (int): The cutoff rank K.\n",
    "\n",
    "    Returns:\n",
    "        float: Recall at K value.\n",
    "    \"\"\"\n",
    "    # Get the top-k retrieved documents\n",
    "    top_k_retrieved = retrieved_docs[:k]\n",
    "    \n",
    "    # Count the number of relevant documents in the top-k retrieved documents\n",
    "    relevant_retrieved_count = sum(1 for doc_id in top_k_retrieved if doc_id in relevant_docs)\n",
    "    \n",
    "    recall_at_k = relevant_retrieved_count / len(relevant_docs) if relevant_docs else 0.0\n",
    "    \n",
    "    return recall_at_k\n",
    "\n",
    "#compute the hit rate at k\n",
    "def compute_hit_rate_at_k(retrieved_docs: list, relevant_docs: list, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute Hit Rate at K for the retrieved documents.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs (list): List of retrieved document IDs.\n",
    "        relevant_docs (list): List of relevant document IDs.\n",
    "        k (int): The cutoff rank K.\n",
    "\n",
    "    Returns:\n",
    "        float: Hit Rate at K value.\n",
    "    \"\"\"\n",
    "    # Get the top-k retrieved documents\n",
    "    top_k_retrieved = retrieved_docs[:k]\n",
    "    \n",
    "    # Check if any relevant document is in the top-k retrieved documents\n",
    "    hit = any(doc_id in relevant_docs for doc_id in top_k_retrieved)\n",
    "    \n",
    "    return 1.0 if hit else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29893cea",
   "metadata": {},
   "source": [
    "**define functions that calculate mean metrics over all queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "b6601491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_metrics(metrics: list) -> float:\n",
    "    \"\"\"\n",
    "    Compute the mean of all metrics.\n",
    "    \n",
    "    Args:\n",
    "        metrics (list): A list of dictionaries containing metric names and their values.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the mean off the metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_metrics = {}\n",
    "    for metric_name in metrics[0].keys():\n",
    "        mean_metrics[f'mean_{metric_name}'] = np.mean([m[metric_name] for m in metrics])\n",
    "    return mean_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca52c29a",
   "metadata": {},
   "source": [
    "**define function that write metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "766887b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_metrics(output_json: dict, top_k: int, emb_col: str, metrics: dict, query_text=None):\n",
    "    \"\"\"\n",
    "    Write the metrics per query to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        output_json (dict): The dictionary to store the metrics.\n",
    "        top_k (int): The value of K for precision/recall at K.\n",
    "        query_text (str, optional): The query text. Defaults to None when we calculate the aggregated metrics over all queries.\n",
    "        emb_col (str): The embedding column used.\n",
    "        metrics (dict): A dictionary representing metric names and their values.\n",
    "    \"\"\"\n",
    "    \n",
    "    if top_k not in output_json:\n",
    "        output_json[top_k] = {}\n",
    "    \n",
    "    if emb_col not in output_json[top_k]:\n",
    "        output_json[top_k][emb_col] = {}\n",
    "    \n",
    "    if query_text is not None:\n",
    "        if query_text not in output_json[top_k][emb_col]:\n",
    "            output_json[top_k][emb_col][query_text] = {}\n",
    "            \n",
    "        for metric_name, value in metrics.items():\n",
    "            output_json[top_k][emb_col][query_text][metric_name] = value  \n",
    "    \n",
    "    #we are aggreating over all queries\n",
    "    else:\n",
    "        for metric_name, value in metrics.items():\n",
    "            output_json[top_k][emb_col][metric_name] = value  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e335d31",
   "metadata": {},
   "source": [
    "**define a function that write retrived documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "2cdf343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write retrived documents to json\n",
    "def store_retrived_documents(retrived_json: dict, top_k: int, query_text: str, emb_col: str) -> None:\n",
    "    \"\"\"\n",
    "    Write the retrieved documents to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        retrived_json (dict): Dictionary of retrieved documents.\n",
    "        top_k (int): The number of top documents retrieved.\n",
    "        query_text (str): The input query string.\n",
    "        emb_col (str): The embedding column name.\n",
    "    \"\"\"\n",
    "    \n",
    "    results_list = []\n",
    "    rank = 1\n",
    "    \n",
    "    for _, row in retrieved_df.iterrows():\n",
    "        results_list.append({\n",
    "            \"RANK\": rank,\n",
    "            \"ID\": row[\"ID\"],\n",
    "            \"NAME\": row[\"NAME_CLEAND\"],\n",
    "            \"TAGS\": row[\"TAGS_CLEAND\"],\n",
    "            \"INGREDIENTS\": row[\"INGREDIENTS_CLEAND\"],\n",
    "            \"STEPS\": row[\"STEPS_CLEAND\"],\n",
    "            \"DESCRIPTION\": row[\"DESCRIPTION_CLEAND\"],\n",
    "            # \"similarity_score\": float(row[\"similarity_score\"])\n",
    "            })\n",
    "        rank += 1\n",
    "            \n",
    "        if top_k not in retrived_json:\n",
    "            retrived_json[top_k] = {}\n",
    "        if emb_col not in retrived_json[top_k]:\n",
    "            retrived_json[top_k][emb_col] = {}\n",
    "        \n",
    "        retrived_json[top_k][emb_col][query_text] = results_list   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2183c193",
   "metadata": {},
   "source": [
    "**get the top_k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "42ba3f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\n"
     ]
    }
   ],
   "source": [
    "TOP_K_LIST = config[\"experiments_specifique_params\"][\"top_k\"]\n",
    "\n",
    "print(TOP_K_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fefce27",
   "metadata": {},
   "source": [
    "**calculate the metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "b9050e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_cols = [col for col in df_recipes_cleaned.columns if col.endswith(\"_EMB\")]\n",
    "\n",
    "output_json_per_query = {}\n",
    "output_json_all_query = {}\n",
    "output_retrived_result_per_query = {}\n",
    "output_test = {}\n",
    "\n",
    "\n",
    "for top_k in TOP_K_LIST:\n",
    "    \n",
    "    for emb_col in emb_cols:\n",
    "            \n",
    "        # Convert embedding column to a list of numpy arrays\n",
    "        documents = df_recipes_cleaned[emb_col].apply(np.array).to_list()\n",
    "        \n",
    "        # Build model name + config name from column name\n",
    "        model_name = \"/\".join(emb_col.split(\"/\")[:-1])   # Snowflake/snowflake-arctic-embed-m\n",
    "        config_name = emb_col.split(\"/\")[-1].replace(\"_EMB\", \"\")\n",
    "        \n",
    "        model = MODEL_DICT[model_name]\n",
    "        \n",
    "        metrics_all_queries = []\n",
    "\n",
    "        for query in query_documents_dicts:\n",
    "            query_text = query['query_text']\n",
    "            document_ids = query['documents']\n",
    "            \n",
    "            retrieved_df = retrieve_documents(query_text, model, documents, df_recipes_cleaned, top_k=top_k)\n",
    "            \n",
    "            if emb_col == \"intfloat/e5-base-v2/config_1_EMB\":\n",
    "                store_retrived_documents(\n",
    "                    retrived_json=output_test,\n",
    "                    top_k=top_k,\n",
    "                    query_text=query_text,\n",
    "                    emb_col=emb_col\n",
    "                )\n",
    "            \n",
    "            #calculate metrics\n",
    "            metrics_per_query = {\n",
    "                'precision_at_k': compute_precision_at_k([doc for doc in retrieved_df['ID'].tolist()], document_ids, top_k),\n",
    "                'recall_at_k': compute_recall_at_k([doc for doc in retrieved_df['ID'].tolist()], document_ids, top_k),\n",
    "                'hit_rate_at_k': compute_hit_rate_at_k([doc for doc in retrieved_df['ID'].tolist()], document_ids, top_k),\n",
    "            }\n",
    "            \n",
    "            #store the metrics per query\n",
    "            metrics_all_queries.append(metrics_per_query)\n",
    "            \n",
    "            #store metrics per query\n",
    "            store_metrics(\n",
    "                output_json=output_json_per_query,\n",
    "                top_k=top_k,\n",
    "                query_text=query_text,\n",
    "                emb_col=emb_col,\n",
    "                metrics=metrics_per_query\n",
    "            )\n",
    "        \n",
    "        with open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(output_test, f, indent=4)\n",
    "            \n",
    "        #claculate mean precision per query\n",
    "        mean_metrics = compute_mean_metrics(metrics=metrics_all_queries)\n",
    "        \n",
    "        #store aggregated metrics over all queries\n",
    "        store_metrics(\n",
    "            output_json=output_json_all_query,\n",
    "            top_k=top_k,\n",
    "            emb_col=emb_col,\n",
    "            metrics=mean_metrics\n",
    "        )\n",
    "\n",
    "with open(RETRIVED_DOCUMENTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_retrived_result_per_query, f, indent=4)\n",
    "\n",
    "# Write all results once at the end\n",
    "with open(METRIC_PER_QUERY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_json_per_query, f, indent=4)\n",
    "    \n",
    "# Write all results once at the end\n",
    "with open(METRIC_QUERY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_json_all_query, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e765349",
   "metadata": {},
   "source": [
    "**write the config specifique file for the experience**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "5fc5189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUPUT_EXPERIMENT_DIR = config[\"output_experiments_dir\"].format(\n",
    "    experiment_id=EXPERIENCE_ID \n",
    ")\n",
    "\n",
    "# Write the config file\n",
    "with open(os.path.join(OUPUT_EXPERIMENT_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
