{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938950b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, Any, List\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988dc537",
   "metadata": {},
   "source": [
    "**read the metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa48fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "CONFIG_FILE_PATH = os.getenv(\"CONFIG_FILE_PATH\")\n",
    "\n",
    "with open(CONFIG_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7951be1",
   "metadata": {},
   "source": [
    "**set the experience id** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a97dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIENCE_ID = config[\"experiments_specifique_params\"][\"experiment_id\"]\n",
    "\n",
    "print(EXPERIENCE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a987c061",
   "metadata": {},
   "source": [
    "**load the metrics files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585111a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_QUERY_FILE = config[\"output_query_metrics_file\"].format(\n",
    "    experiment_id=EXPERIENCE_ID \n",
    ")\n",
    "\n",
    "print(METRIC_QUERY_FILE)\n",
    "\n",
    "with open(METRIC_QUERY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    metrics_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bbadd0",
   "metadata": {},
   "source": [
    "**define a function that return dict of metrcis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1688f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metrics(metrics_json: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Reformats the nested metrics JSON into a flat list of dictionaries, \n",
    "    making it easier to process and plot.\n",
    "\n",
    "    Args:\n",
    "        metrics_json: The loaded JSON object (dictionary) containing \n",
    "                      metrics keyed by 'k' value as a string.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents \n",
    "        a single (k, config, metric) data point.\n",
    "    \"\"\"\n",
    "    reformatted_data = []\n",
    "    \n",
    "    # Iterate through the top-level keys, which are the k values (as strings)\n",
    "    for k_str, config_data in metrics_json.items():\n",
    "        try:\n",
    "            k = int(k_str)  # Convert the k value back to an integer\n",
    "        except ValueError:\n",
    "            print(f\"Skipping key that cannot be converted to integer k: {k_str}\")\n",
    "            continue\n",
    "        \n",
    "        # Iterate through the config/model names within that k\n",
    "        for config_name, metrics in config_data.items():\n",
    "            \n",
    "            # Create a dictionary with only the desired fields\n",
    "            print(metrics)\n",
    "            data_point = {\n",
    "                \"k\": k,\n",
    "                \"config\": config_name,\n",
    "                \"precision\": metrics.get(\"mean_precision_at_k\", 0.0),\n",
    "                \"recall\": metrics.get(\"mean_recall_at_k\", 0.0),\n",
    "                \"hit_rate\": metrics.get(\"mean_hit_rate_at_k\", 0.0)\n",
    "            }\n",
    "            reformatted_data.append(data_point)\n",
    "            \n",
    "    return reformatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = format_metrics(metrics_json)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ca2c2",
   "metadata": {},
   "source": [
    "**load the folder to store the images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_METRIC_PLOTS_DIR = config[\"output_retrival_plot_metrics_dir\"].format(\n",
    "    experiment_id=EXPERIENCE_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc1db7",
   "metadata": {},
   "source": [
    "**define a function to plot the metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb03f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(reformatted_data: List[Dict[str, Any]], output_dir: str = 'model_comparisons'):\n",
    "    \"\"\"\n",
    "    Generates a separate plot for each unique Config ID, showing all model \n",
    "    performances within that configuration, organizing plots into subfolders \n",
    "    by metric type (precision/recall).\n",
    "\n",
    "    Args:\n",
    "        reformatted_data: The flat list of dictionaries with k, config, precision, and recall.\n",
    "        output_dir: The base directory where the plots will be saved. Created if it doesn't exist.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(reformatted_data)\n",
    "\n",
    "    # Create the base output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Saving plots to base directory: {output_dir}\")\n",
    "    \n",
    "    # --- 1. Label and Group Extraction ---\n",
    "    def parse_config(config_str):\n",
    "        parts = config_str.split('/')\n",
    "        model_name = parts[1]\n",
    "        full_config_part = parts[-1]\n",
    "        return model_name, full_config_part\n",
    "\n",
    "    df[['model_name', 'full_config_id']] = df['config'].apply(\n",
    "        lambda x: pd.Series(parse_config(x))\n",
    "    )\n",
    "    \n",
    "    unique_config_ids = df['full_config_id'].unique()\n",
    "    unique_models = df['model_name'].unique()\n",
    "    \n",
    "    # --- 2. Color Mapping ---\n",
    "    model_colors = plt.cm.get_cmap('tab10', len(unique_models))\n",
    "    model_color_map = {model: model_colors(i) for i, model in enumerate(unique_models)}\n",
    "\n",
    "    # --- 3. Plotting Loop (One plot per Config ID) ---\n",
    "    for config_id in sorted(unique_config_ids): \n",
    "    \n",
    "        config_data_subset = df[df['full_config_id'] == config_id].copy() \n",
    "        \n",
    "        for metric_type in ['precision', 'recall', 'hit_rate']:\n",
    "            \n",
    "            # --- CREATE METRIC SUBFOLDER HERE ---\n",
    "            metric_dir = os.path.join(output_dir, metric_type)\n",
    "            os.makedirs(metric_dir, exist_ok=True)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            \n",
    "            for model in unique_models:\n",
    "                model_data = config_data_subset[config_data_subset['model_name'] == model]\n",
    "                \n",
    "                if not model_data.empty:\n",
    "                    ax.plot(\n",
    "                        model_data['k'], \n",
    "                        model_data[metric_type], \n",
    "                        marker='o', \n",
    "                        linestyle='-', \n",
    "                        label=model,\n",
    "                        color=model_color_map[model],\n",
    "                        linewidth=2\n",
    "                    )\n",
    "            \n",
    "            metric_title = metric_type.capitalize()\n",
    "            \n",
    "            # --- Set Plot Titles and Labels ---\n",
    "            ax.set_title(f'Model Comparison for {config_id}: Mean {metric_title}@k', fontsize=14)\n",
    "            ax.set_xlabel('k (Number of Retrieved Items)', fontsize=12)\n",
    "            ax.set_ylabel(f'Mean {metric_title}@k', fontsize=12)\n",
    "            ax.set_ylim(-0.05, 1.05)\n",
    "            \n",
    "            ax.legend(\n",
    "                title='Model', \n",
    "                loc='upper right', \n",
    "                fontsize='small'\n",
    "            ) \n",
    "            ax.grid(True, linestyle='--', alpha=0.6)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the file into the metric-specific subdirectory\n",
    "            filename = f\"vs_k_{config_id}_comparison.png\" # Removed metric_type from filename\n",
    "            full_path = os.path.join(metric_dir, filename)\n",
    "            plt.savefig(full_path)\n",
    "            plt.close(fig)\n",
    "            print(f\"Generated plot: {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38791c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics, output_dir=OUTPUT_METRIC_PLOTS_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
