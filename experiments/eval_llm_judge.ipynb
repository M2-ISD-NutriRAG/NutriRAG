{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99083e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "original_cwd = os.getcwd()\n",
    "\n",
    "backend_path = os.path.abspath(os.path.join(original_cwd, \"../backend\"))\n",
    "added_backend = False\n",
    "\n",
    "if not any(\"backend\" in p for p in sys.path):\n",
    "    sys.path.insert(0, backend_path)\n",
    "    added_backend = True\n",
    "    print(f\"Added backend to sys.path: {backend_path}\")\n",
    "else:\n",
    "    print(\"Backend already in sys.path, skipping.\")\n",
    "\n",
    "from shared.snowflake.client import SnowflakeClient\n",
    "\n",
    "os.chdir(original_cwd)\n",
    "print(f\"Returned to original working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c88417b",
   "metadata": {},
   "source": [
    "**load the config file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2208ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = \"config/base_config.json\"\n",
    "\n",
    "with open(CONFIG_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7a230",
   "metadata": {},
   "source": [
    "**set the experience id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23fa4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIENCE_ID = config[\"experiments_specifique_params\"][\"experiment_id\"]\n",
    "\n",
    "print(EXPERIENCE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c13ca",
   "metadata": {},
   "source": [
    "**load the embedding file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9260510",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_EMBEDDINGS_FILE = config[\"output_recipies_embedding_file\"].format(\n",
    "    experiment_id=EXPERIENCE_ID\n",
    ")\n",
    "df_recipes_cleaned = pd.read_csv(INPUT_EMBEDDINGS_FILE)\n",
    "\n",
    "\n",
    "emb_columns = [col for col in df_recipes_cleaned.columns if col.endswith('_EMB')]\n",
    "print(f\"Found embedding columns: {emb_columns}\")\n",
    "\n",
    "for col in emb_columns:\n",
    "    df_recipes_cleaned[col] = df_recipes_cleaned[col].apply(\n",
    "        lambda x: np.fromstring(x.strip('[]'), sep=' ', dtype=np.float32)\n",
    "    )\n",
    "\n",
    "for col in emb_columns:\n",
    "    print(f\"{col} -> first embedding shape: {df_recipes_cleaned[col][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19e6462",
   "metadata": {},
   "source": [
    "## evaluation the LLM and the prompt system using the ground truth ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ccdf7",
   "metadata": {},
   "source": [
    "**define function that call llm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b0f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(client: SnowflakeClient, model: str, prompt: str, response_format: dict) -> str:\n",
    "    \"\"\"\n",
    "    Query Snowflake LLM with JSON schema for structured output.\n",
    "    \n",
    "    Args:\n",
    "        client: SnowflakeClient instance\n",
    "        model: Model name (e.g., 'mistral-large2')\n",
    "        prompt: The prompt text\n",
    "        response_format: JSON schema dict\n",
    "        \n",
    "    Returns:\n",
    "        str: JSON response from LLM\n",
    "    \"\"\"\n",
    "    \n",
    "    query = \"\"\"\n",
    "        SELECT AI_COMPLETE(\n",
    "            model => %s,\n",
    "            prompt => %s,\n",
    "            response_format => PARSE_JSON(%s)\n",
    "        ) AS response;\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format the response_format as Snowflake expects\n",
    "    response_format_json = {\n",
    "        'type': 'json',\n",
    "        'schema': response_format\n",
    "    }\n",
    "    \n",
    "    result = client.execute(\n",
    "        query, \n",
    "        params=(model, prompt, json.dumps(response_format_json)), \n",
    "        fetch=\"one\"\n",
    "    )\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee59014",
   "metadata": {},
   "source": [
    "**load the test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eea0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_TEST_FILE_PATH = config[\"query_test_file_path\"]\n",
    "\n",
    "with open(QUERY_TEST_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    query_documents_dicts = json.load(f)\n",
    "\n",
    "print(QUERY_TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682456b",
   "metadata": {},
   "source": [
    "**load the prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3526944",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_EVAL_PATH = config[\"eval_prompt_file\"]\n",
    "\n",
    "with open(PROMPT_EVAL_PATH, \"r\") as f:\n",
    "    prompt_template = f.read()\n",
    "    \n",
    "def build_prompt(query_text, doc_entries):\n",
    "    # doc_entries should be a python list/dict → convert to JSON text\n",
    "    doc_json = json.dumps(doc_entries, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # DO SAFE REPLACEMENT (no .format()!)\n",
    "    prompt = (\n",
    "        prompt_template\n",
    "        .replace(\"{input_query_text}\", query_text)\n",
    "        .replace(\"{doc_entries}\", doc_json)\n",
    "    )\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2274d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON schema for recipe relevance ratings\n",
    "json_schema = {\n",
    "    'type': 'object',\n",
    "    'properties': {\n",
    "        'query_text': {\n",
    "            'type': 'string'\n",
    "        },\n",
    "        'relevance_judgments': {\n",
    "            'type': 'array',\n",
    "            'items': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'doc_id': {'type': 'integer'},\n",
    "                    'relevance_score': {'type': 'number'}\n",
    "                },\n",
    "                'required': ['doc_id', 'relevance_score']\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'required': ['query_text', 'relevance_judgments']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1cb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "COLUMNS_TEXT = config[\"columns_to_clean\"]\n",
    "LLM_MODEL = config[\"llm_model\"]\n",
    "\n",
    "def normalize_json_response(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize JSON response to compact single line, preserving spaces inside string values.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw JSON string from LLM (may contain \\n, extra spaces, etc.)\n",
    "        \n",
    "    Returns:\n",
    "        str: Compact, single-line JSON string\n",
    "    \"\"\"\n",
    "    # Remove all literal \\n escape sequences\n",
    "    text = text.replace('\\\\n', '')\n",
    "    \n",
    "    # Remove actual newlines\n",
    "    text = text.replace('\\n', '').replace('\\r', '')\n",
    "    \n",
    "    # Remove markdown code blocks\n",
    "    text = re.sub(r'```(?:json)?', '', text).strip('` ')\n",
    "    \n",
    "    # Remove unwanted \"\"\n",
    "    text = text.strip().strip('\"').strip(\"'\")\n",
    "    \n",
    "    try:\n",
    "        parsed = json.loads(text)\n",
    "        return json.dumps(parsed, separators=(',', ':'), ensure_ascii=False)\n",
    "    except json.JSONDecodeError:\n",
    "        text = re.sub(r'\\s*:\\s*', ':', text)\n",
    "        text = re.sub(r'\\s*,\\s*', ',', text)\n",
    "        text = re.sub(r'\\{\\s+', '{', text)\n",
    "        text = re.sub(r'\\s+\\}', '}', text)\n",
    "        text = re.sub(r'\\[\\s+', '[', text)\n",
    "        text = re.sub(r'\\s+\\]', ']', text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "\n",
    "ratings_results = []\n",
    "\n",
    "for query in query_documents_dicts:\n",
    "    query_text = query[\"query_text\"]\n",
    "\n",
    "    # Build doc entries for the prompt\n",
    "    doc_entries = []\n",
    "    for document in query[\"relevance_documents\"]:\n",
    "        doc_id = document['doc_id']\n",
    "        recipe_row = df_recipes_cleaned[df_recipes_cleaned[\"ID\"] == doc_id]\n",
    "        if recipe_row.empty:\n",
    "            continue\n",
    "\n",
    "        recipe_info = {}\n",
    "        for col_key, col_info in COLUMNS_TEXT.items():\n",
    "            recipe_info[col_info[\"start_text\"]] = recipe_row.iloc[0][col_info[\"column_name\"]]\n",
    "\n",
    "        doc_entries.append({\"doc_id\": doc_id, \"recipe_info\": recipe_info})\n",
    "\n",
    "    # Create the prompt\n",
    "    prompt = build_prompt(query_text, doc_entries)\n",
    "\n",
    "    # Call the LLM\n",
    "    llm_response = get_llm_response(client=SnowflakeClient(), model=LLM_MODEL, prompt=prompt, response_format=json_schema)\n",
    "\n",
    "    # Extract and parse JSON (this now returns a proper dict, not a string)\n",
    "    llm_response_clean = normalize_json_response(llm_response)\n",
    "    json_output = json.loads(llm_response_clean)\n",
    "    print(json_output)\n",
    "    \n",
    "    ratings_results.append(json_output)\n",
    "    print(f\"✓ Parsed JSON for query: {query_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc543ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ratings_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f749477",
   "metadata": {},
   "source": [
    "## compare LLM response to the ground truth ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602b3d5",
   "metadata": {},
   "source": [
    "**compare the LLM response with the ground truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ground_truth_vs_llm(ground_truth: dict, llm_results: dict) -> tuple[int, list]:\n",
    "    \"\"\"\n",
    "    Computes how close LLM relevance scores are to ground truth.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth(dict): dictionnary containing for each query relevent documents\n",
    "        llm_results(dict): dictionnary containing for each query a relevent score for the doc in the ground truth\n",
    "        \n",
    "    Return\n",
    "    \"\"\"\n",
    "\n",
    "    query_diffs = []\n",
    "\n",
    "    for gt_query, llm_query in zip(ground_truth, llm_results):\n",
    "\n",
    "        # Convert lists to dict {doc_id: score}\n",
    "        gt_scores = {d[\"doc_id\"]: d[\"relevance_score\"] for d in gt_query[\"relevance_documents\"]}\n",
    "        print(llm_query)\n",
    "        llm_scores = {d[\"doc_id\"]: d[\"relevance_score\"] for d in llm_query[\"relevance_judgments\"]}\n",
    "\n",
    "        # Use union of doc_ids so nothing is skipped\n",
    "        all_doc_ids = set(gt_scores.keys()) | set(llm_scores.keys())\n",
    "\n",
    "        diffs = []\n",
    "        for doc_id in all_doc_ids:\n",
    "            gt = gt_scores.get(doc_id, 0)   # missing → assume 0\n",
    "            llm = llm_scores.get(doc_id, 0)\n",
    "            diffs.append(1 - abs(gt - llm))\n",
    "\n",
    "        # average difference for this query\n",
    "        query_diffs.append(sum(diffs) / len(diffs))\n",
    "\n",
    "    # overall average difference across all queries\n",
    "    final_score = sum(query_diffs) / len(query_diffs)\n",
    "    return final_score, query_diffs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091a3532",
   "metadata": {},
   "source": [
    "**calculate coherence between LLM and test (ground truth)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e2454",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_LLM_RESULTS_PATH = config['query_llm_file_path'].format(\n",
    "    experiment_id=EXPERIENCE_ID \n",
    ")\n",
    "\n",
    "final_score, per_query_scores = compare_ground_truth_vs_llm(query_documents_dicts, ratings_results)\n",
    "\n",
    "print(\"Final coherence score:\", final_score)\n",
    "print(\"Per-query coherence:\", per_query_scores)\n",
    "\n",
    "ratings_results.append({'COHERENCE_SCORE': final_score})\n",
    "\n",
    "# Save results - now they're proper dicts, not strings\n",
    "with open(QUERY_LLM_RESULTS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ratings_results, f, indent=2, ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d964498",
   "metadata": {},
   "source": [
    "**write the config file for that experience**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "529a5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUPUT_EXPERIMENT_DIR = config[\"output_experiments_dir\"].format(\n",
    "    experiment_id=EXPERIENCE_ID \n",
    ")\n",
    "\n",
    "# Write the config file\n",
    "with open(os.path.join(OUPUT_EXPERIMENT_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
