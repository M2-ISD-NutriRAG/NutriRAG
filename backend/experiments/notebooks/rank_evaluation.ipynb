{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8774ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9f761",
   "metadata": {},
   "source": [
    "**load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = \"../experiments/exp2/metrics/embedding/retrieved_query_documents_aggregated_metrics.json\"\n",
    "\n",
    "with open(metrics, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147d0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "k_values = sorted(df['k'].unique())\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"RANKING MODELS BY METRICS FOR EACH K\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for k in k_values:\n",
    "    # Filter data for current k\n",
    "    k_data = df[df['k'] == k].copy()\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"K = {k}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Rank by Precision@k\n",
    "    print(f\"\\nüìä RANKED BY PRECISION@{k}:\")\n",
    "    print(\"-\" * 80)\n",
    "    precision_ranked = k_data.sort_values('precision@k', ascending=False)\n",
    "    for idx, row in precision_ranked.iterrows():\n",
    "        print(f\"  {row['model']:30} | {row['config']:15} | Precision@{k}: {row['precision@k']:.4f}\")\n",
    "    \n",
    "    # Rank by Recall@k\n",
    "    print(f\"\\nüìä RANKED BY RECALL@{k}:\")\n",
    "    print(\"-\" * 80)\n",
    "    recall_ranked = k_data.sort_values('recall@k', ascending=False)\n",
    "    for idx, row in recall_ranked.iterrows():\n",
    "        print(f\"  {row['model']:30} | {row['config']:15} | Recall@{k}: {row['recall@k']:.4f}\")\n",
    "    \n",
    "    # Rank by NDCG@k\n",
    "    print(f\"\\nüìä RANKED BY NDCG@{k}:\")\n",
    "    print(\"-\" * 80)\n",
    "    ndcg_ranked = k_data.sort_values('NDCG@k', ascending=False)\n",
    "    for idx, row in ndcg_ranked.iterrows():\n",
    "        print(f\"  {row['model']:30} | {row['config']:15} | NDCG@{k}: {row['NDCG@k']:.4f}\")\n",
    "    \n",
    "    # Rank by MAP@k\n",
    "    print(f\"\\nüìä RANKED BY MAP@{k}:\")\n",
    "    print(\"-\" * 80)\n",
    "    map_ranked = k_data.sort_values('MAP@k', ascending=False)\n",
    "    for idx, row in map_ranked.iterrows():\n",
    "        print(f\"  {row['model']:30} | {row['config']:15} | MAP@{k}: {row['MAP@k']:.4f}\")\n",
    "    \n",
    "    # Rank by MRR@ka\n",
    "    print(f\"\\nüìä RANKED BY MRR@{k}:\")\n",
    "    print(\"-\" * 80)\n",
    "    mrr_ranked = k_data.sort_values('MRR@k', ascending=False)\n",
    "    for idx, row in mrr_ranked.iterrows():\n",
    "        print(f\"  {row['model']:30} | {row['config']:15} | MRR@{k}: {row['MRR@k']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"SUMMARY: TOP PERFORMERS ACROSS ALL K VALUES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Overall best by each metric (averaged across all k)\n",
    "overall_avg = df.groupby(['model', 'config']).agg({\n",
    "    'precision@k': 'mean',\n",
    "    'recall@k': 'mean',\n",
    "    'NDCG@k': 'mean',\n",
    "    'MAP@k': 'mean',\n",
    "    'MRR@k': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "print(\"\\nüèÜ BEST AVERAGE PRECISION:\")\n",
    "best_precision = overall_avg.nlargest(3, 'precision@k')\n",
    "for idx, row in best_precision.iterrows():\n",
    "    print(f\"  {row['model']:30} | {row['config']:15} | Avg Precision: {row['precision@k']:.4f}\")\n",
    "\n",
    "print(\"\\nüèÜ BEST AVERAGE RECALL:\")\n",
    "best_recall = overall_avg.nlargest(3, 'recall@k')\n",
    "for idx, row in best_recall.iterrows():\n",
    "    print(f\"  {row['model']:30} | {row['config']:15} | Avg Recall: {row['recall@k']:.4f}\")\n",
    "\n",
    "print(\"\\nüèÜ BEST AVERAGE NDCG:\")\n",
    "best_ndcg = overall_avg.nlargest(3, 'NDCG@k')\n",
    "for idx, row in best_ndcg.iterrows():\n",
    "    print(f\"  {row['model']:30} | {row['config']:15} | Avg NDCG: {row['NDCG@k']:.4f}\")\n",
    "\n",
    "print(\"\\nüèÜ BEST AVERAGE MAP:\")\n",
    "best_map = overall_avg.nlargest(3, 'MAP@k')\n",
    "for idx, row in best_map.iterrows():\n",
    "    print(f\"  {row['model']:30} | {row['config']:15} | Avg MAP: {row['MAP@k']:.4f}\")\n",
    "\n",
    "print(\"\\nüèÜ BEST AVERAGE MRR:\")\n",
    "best_mrr = overall_avg.nlargest(3, 'MRR@k')\n",
    "for idx, row in best_mrr.iterrows():\n",
    "    print(f\"  {row['model']:30} | {row['config']:15} | Avg MRR: {row['MRR@k']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99186594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "metrics = \"../experiments/exp2/metrics/embedding/retrieved_query_documents_aggregated_metrics.json\"\n",
    "\n",
    "with open(metrics, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "k_values = sorted(df['k'].unique())\n",
    "\n",
    "for k in k_values:\n",
    "    k_data = df[df['k'] == k].copy()\n",
    "    k_data = k_data.sort_values('NDCG@k', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{'='*120}\")\n",
    "    print(f\"K = {k} - ALL METRICS (Sorted by NDCG@{k})\")\n",
    "    print(f\"{'='*120}\")\n",
    "    print(f\"{'Model':<30} | {'Config':<12} | {'Prec@k':>8} | {'Recall@k':>10} | {'MAP@k':>8} | {'NDCG@k':>8} | {'MRR@k':>8}\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    for idx, row in k_data.iterrows():\n",
    "        print(f\"{row['model']:<30} | {row['config']:<12} | \"\n",
    "              f\"{row['precision@k']:>8.4f} | {row['recall@k']:>10.4f} | \"\n",
    "              f\"{row['MAP@k']:>8.4f} | {row['NDCG@k']:>8.4f} | {row['MRR@k']:>8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e6d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_eval = \"../experiments/exp2/data/temp/retrieved_query_documents_relevance.json\"\n",
    "\n",
    "with open(metrics_eval, \"r\", encoding=\"utf-8\") as f:\n",
    "    data_eval = json.load(f)\n",
    "    \n",
    "print(data_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_relevance_scores(data_eval):\n",
    "    \"\"\"\n",
    "    Calculate average number of documents with relevance scores 1 and 0.5 across all queries.\n",
    "    \n",
    "    Args:\n",
    "        data_eval: Dictionary mapping queries to document relevance scores\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with statistics\n",
    "    \"\"\"\n",
    "    total_queries = len(data_eval)\n",
    "    total_rel_1 = 0\n",
    "    total_rel_0_5 = 0\n",
    "    total_rel_0 = 0\n",
    "    \n",
    "    query_stats = []\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(\"RELEVANCE SCORE ANALYSIS\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for query, docs in data_eval.items():\n",
    "        # Count documents by relevance score\n",
    "        count_rel_1 = sum(1 for score in docs.values() if score == 1 or score == 1.0)\n",
    "        count_rel_0_5 = sum(1 for score in docs.values() if score == 0.5)\n",
    "        count_rel_0 = sum(1 for score in docs.values() if score == 0 or score == 0.0)\n",
    "        \n",
    "        total_rel_1 += count_rel_1\n",
    "        total_rel_0_5 += count_rel_0_5\n",
    "        total_rel_0 += count_rel_0\n",
    "        \n",
    "        query_stats.append({\n",
    "            'query': query,\n",
    "            'rel_1': count_rel_1,\n",
    "            'rel_0.5': count_rel_0_5,\n",
    "            'rel_0': count_rel_0,\n",
    "            'total_docs': len(docs)\n",
    "        })\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_rel_1 = total_rel_1 / total_queries\n",
    "    avg_rel_0_5 = total_rel_0_5 / total_queries\n",
    "    avg_rel_0 = total_rel_0 / total_queries\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nTotal Queries: {total_queries}\")\n",
    "    print(\"\\n\" + \"-\" * 100)\n",
    "    print(\"AVERAGE STATISTICS:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Average documents with relevance = 1.0:   {avg_rel_1:.2f}\")\n",
    "    print(f\"Average documents with relevance = 0.5:   {avg_rel_0_5:.2f}\")\n",
    "    print(f\"Average documents with relevance = 0.0:   {avg_rel_0:.2f}\")\n",
    "    \n",
    "    # Print detailed per-query statistics\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"PER-QUERY BREAKDOWN:\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"{'Query':<60} | {'Rel=1':<8} | {'Rel=0.5':<8} | {'Rel=0':<8} | {'Total':<8}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for stat in query_stats:\n",
    "        query_short = stat['query'][:57] + \"...\" if len(stat['query']) > 60 else stat['query']\n",
    "        print(f\"{query_short:<60} | {stat['rel_1']:<8} | {stat['rel_0.5']:<8} | {stat['rel_0']:<8} | {stat['total_docs']:<8}\")\n",
    "    \n",
    "    # Distribution analysis\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"DISTRIBUTION ANALYSIS:\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    queries_with_no_relevant = sum(1 for stat in query_stats if stat['rel_1'] == 0 and stat['rel_0.5'] == 0)\n",
    "    queries_with_1_5_relevant = sum(1 for stat in query_stats if 1 <= (stat['rel_1'] + stat['rel_0.5']) <= 5)\n",
    "    queries_with_5_10_relevant = sum(1 for stat in query_stats if 5 < (stat['rel_1'] + stat['rel_0.5']) <= 10)\n",
    "    queries_with_10_plus_relevant = sum(1 for stat in query_stats if (stat['rel_1'] + stat['rel_0.5']) > 10)\n",
    "    \n",
    "    print(f\"Queries with 0 relevant documents:       {queries_with_no_relevant} ({queries_with_no_relevant/total_queries*100:.1f}%)\")\n",
    "    print(f\"Queries with 1-5 relevant documents:     {queries_with_1_5_relevant} ({queries_with_1_5_relevant/total_queries*100:.1f}%)\")\n",
    "    print(f\"Queries with 5-10 relevant documents:    {queries_with_5_10_relevant} ({queries_with_5_10_relevant/total_queries*100:.1f}%)\")\n",
    "    print(f\"Queries with 10+ relevant documents:     {queries_with_10_plus_relevant} ({queries_with_10_plus_relevant/total_queries*100:.1f}%)\")\n",
    "    \n",
    "    # Find queries with most/least relevant docs\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"EXTREME CASES:\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    sorted_by_relevant = sorted(query_stats, key=lambda x: x['rel_1'] + x['rel_0.5'], reverse=True)\n",
    "    \n",
    "    print(\"\\nüìä Top 5 queries with MOST relevant documents:\")\n",
    "    for i, stat in enumerate(sorted_by_relevant[:5], 1):\n",
    "        total_relevant = stat['rel_1'] + stat['rel_0.5']\n",
    "        print(f\"{i}. {stat['query'][:70]}\")\n",
    "        print(f\"   ‚Üí Rel=1: {stat['rel_1']}, Rel=0.5: {stat['rel_0.5']}, Total relevant: {total_relevant}\")\n",
    "    \n",
    "    print(\"\\nüìä Top 5 queries with LEAST relevant documents:\")\n",
    "    for i, stat in enumerate(sorted_by_relevant[-5:], 1):\n",
    "        total_relevant = stat['rel_1'] + stat['rel_0.5']\n",
    "        print(f\"{i}. {stat['query'][:70]}\")\n",
    "        print(f\"   ‚Üí Rel=1: {stat['rel_1']}, Rel=0.5: {stat['rel_0.5']}, Total relevant: {total_relevant}\")\n",
    "    \n",
    "    return {\n",
    "        'total_queries': total_queries,\n",
    "        'avg_rel_1': avg_rel_1,\n",
    "        'avg_rel_0_5': avg_rel_0_5,\n",
    "        'avg_rel_0': avg_rel_0,\n",
    "        'total_rel_1': total_rel_1,\n",
    "        'total_rel_0_5': total_rel_0_5,\n",
    "        'total_rel_0': total_rel_0,\n",
    "        'query_stats': query_stats\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data_eval here\n",
    "    # For the example, you would use the data from the document\n",
    "    \n",
    "    results = analyze_relevance_scores(data_eval)\n",
    "    \n",
    "    # Save results to JSON if needed\n",
    "    with open('relevance_analysis_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(\"\\n‚úÖ Analysis complete! Results saved to 'relevance_analysis_results.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1cdc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "metrics = \"../experiments/exp2/metrics/embedding/retrieved_query_documents_aggregated_metrics.json\"\n",
    "\n",
    "with open(metrics, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "k_values = sorted(df['k'].unique())\n",
    "\n",
    "df['score'] = 0.6 * df['precision@k'] + 0.4 * df['NDCG@k']\n",
    "\n",
    "df[df['k'] == 5][['model', 'config', 'precision@k', 'NDCG@k', 'recall@k', 'score']].sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['k'] == 10][['model', 'config', 'precision@k', 'NDCG@k', 'score']].sort_values('score', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
