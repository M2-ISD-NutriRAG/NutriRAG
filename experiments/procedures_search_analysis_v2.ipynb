{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efe0fc6-5d29-42f8-b990-318b45bde4ad",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.types import StringType, StructType, StructField\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Table & Index Names (Based on your docs)\n",
    "SOURCE_TABLE = \"ENRICHED.RECIPES_SAMPLE_50K\"\n",
    "BM25_INDEX = \"VECTORS.RECIPES_SAMPLE_50K_BM25_INDEX\"\n",
    "EMBEDDINGS_TABLE = \"VECTORS.RECIPES_50K_EMBEDDINGS\"\n",
    "\n",
    "# Procedure Names\n",
    "PROC_BM25 = \"NUTRIRAG_PROJECT.VECTORS.SEARCH_BM25\"\n",
    "PROC_SEMANTIC = \"NUTRIRAG_PROJECT.VECTORS.SEARCH_SEMANTIC\"\n",
    "PROC_HYBRID = \"NUTRIRAG_PROJECT.VECTORS.SEARCH_SIMILAR_RECIPES\"\n",
    "\n",
    "# Hybrid Weights\n",
    "VECTOR_WEIGHT = 0.7\n",
    "BM25_WEIGHT = 0.3\n",
    "\n",
    "# Cortex Model for Evaluation (LLM Judge)\n",
    "JUDGE_MODEL = \"llama3.1-70b\" # or 'mistral-large' <- trés couteux ce truc ptn\n",
    "\n",
    "# Test Parameters\n",
    "TOP_K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3715ff-9e3a-4832-ae66-12f2aeba5930",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"chocolate cake\",                     # Keyword heavy\n",
    "    \"healthy vegetarian dinner quick\",    # Semantic + Constraint\n",
    "    \"gluten free pasta\",   \n",
    "    \"refreshing summer dessert lemon\",    # Semantic / Vibe\n",
    "    \"traditional italian lasagna\",        # Specific entity\n",
    "    \"high protein breakfast\",     \n",
    "    \"comfort food for rainy day\",         # Purely semantic/abstract\n",
    "    \"romantic dinner for two italian style\",  # Vibe + Cuisine\n",
    "    \"something easy to digest when sick\",     # Fonctionnel (BM25 va chercher le mot \"sick\"...)\n",
    "    \"meal to impress my boss\",                # Social / Abstrait\n",
    "    \"post workout high protein recovery\",     # Fonctionnel / Nutrition\n",
    "    \"cozy comfort food for winter night\",     # Vibe / Saison\n",
    "    \"light lunch that won't make me sleepy\",  # Effet désiré\n",
    "    \"kids friendly vegetables dish\"           # Cible\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b784874-ed34-4969-bc1d-497e50bd0e59",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "def run_bm25(query):\n",
    "    \"\"\"Executes BM25 Search Procedure\"\"\"\n",
    "    try:\n",
    "        # Args: Query, IndexTable, SourceTable, Limit, Filters\n",
    "        # Note: Filters are NULL (None) for this benchmark\n",
    "        res = session.call(PROC_BM25, query, BM25_INDEX, SOURCE_TABLE, TOP_K, None)\n",
    "        return json.loads(res) if isinstance(res, str) else res\n",
    "    except Exception as e:\n",
    "        print(f\"Error in BM25: {e}\")\n",
    "        return []\n",
    "\n",
    "def run_semantic(query):\n",
    "    \"\"\"Executes Semantic Search Procedure\"\"\"\n",
    "    try:\n",
    "        # Args: Query, EmbeddingsTable, Limit, Filters\n",
    "        res = session.call(PROC_SEMANTIC, query, EMBEDDINGS_TABLE, TOP_K, None)\n",
    "        return json.loads(res) if isinstance(res, str) else res\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Semantic: {e}\")\n",
    "        return []\n",
    "\n",
    "def run_hybrid(query):\n",
    "    \"\"\"Executes Hybrid Search Procedure\"\"\"\n",
    "    try:\n",
    "        # Args: Query, Limit, Filters, VecWeight, BM25Weight, Index, Source, Embeddings\n",
    "        res = session.call(\n",
    "            PROC_HYBRID, \n",
    "            query, \n",
    "            TOP_K, \n",
    "            None, \n",
    "            VECTOR_WEIGHT, \n",
    "            BM25_WEIGHT, \n",
    "            BM25_INDEX, \n",
    "            SOURCE_TABLE, \n",
    "            EMBEDDINGS_TABLE\n",
    "        )\n",
    "        return json.loads(res) if isinstance(res, str) else res\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Hybrid: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90910f1-dbeb-4115-a14f-610e915f39e6",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "results_data = []\n",
    "\n",
    "print(f\"Starting benchmark for {len(test_queries)} queries...\")\n",
    "\n",
    "for q in test_queries:\n",
    "    # 1. BM25\n",
    "    start = time.time()\n",
    "    res_bm25 = run_bm25(q)\n",
    "    dur_bm25 = (time.time() - start) * 1000 # ms\n",
    "    \n",
    "    # 2. Semantic\n",
    "    start = time.time()\n",
    "    res_sem = run_semantic(q)\n",
    "    dur_sem = (time.time() - start) * 1000 # ms\n",
    "    \n",
    "    # 3. Hybrid\n",
    "    start = time.time()\n",
    "    res_hyb = run_hybrid(q)\n",
    "    dur_hyb = (time.time() - start) * 1000 # ms\n",
    "    \n",
    "    # Store raw results for the LLM Judge\n",
    "    results_data.append({\n",
    "        \"query\": q,\n",
    "        \"bm25_res\": res_bm25,\n",
    "        \"bm25_time\": dur_bm25,\n",
    "        \"sem_res\": res_sem,\n",
    "        \"sem_time\": dur_sem,\n",
    "        \"hyb_res\": res_hyb,\n",
    "        \"hyb_time\": dur_hyb\n",
    "    })\n",
    "\n",
    "print(\"Benchmark execution complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae69195-a747-4063-b558-92860347011e",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "for i, row in enumerate(results_data):\n",
    "    print(f\"\\n==================================================\")\n",
    "    print(f\"QUERY {i+1}: '{row['query']}'\")\n",
    "    print(f\"==================================================\")\n",
    "    \n",
    "    # Extract just the top 3 recipe names for clarity\n",
    "    bm25_names = [r.get('NAME', 'Unknown') for r in row['bm25_res'][:3]]\n",
    "    sem_names = [r.get('NAME', 'Unknown') for r in row['sem_res'][:3]]\n",
    "    hyb_names = [r.get('NAME', 'Unknown') for r in row['hyb_res'][:3]]\n",
    "    \n",
    "    # Print comparison\n",
    "    print(f\"--- BM25 Found: ---\")\n",
    "    for name in bm25_names: print(f\"  • {name}\")\n",
    "        \n",
    "    print(f\"\\n--- SEMANTIC Found: ---\")\n",
    "    for name in sem_names: print(f\"  • {name}\")\n",
    "        \n",
    "    print(f\"\\n--- HYBRID Found: ---\")\n",
    "    for name in hyb_names: print(f\"  • {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e45423-eb89-40b6-9cbb-66789a8499a1",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Tu es un expert en recherche d’information (Information Retrieval) et en évaluation de moteurs de recherche.\n",
    "Ton objectif est d’évaluer objectivement la qualité des résultats fournis par trois systèmes différents pour une même requête utilisateur.\n",
    "\n",
    "=== REQUÊTE UTILISATEUR ===\n",
    "{query_text}\n",
    "\n",
    "=== RÉSULTATS DU SYSTÈME A (BM25) ===\n",
    "{bm25_docs}\n",
    "\n",
    "=== RÉSULTATS DU SYSTÈME B (SEMANTIQUE) ===\n",
    "{sem_docs}\n",
    "\n",
    "=== RÉSULTATS DU SYSTÈME C (HYBRIDE) ===\n",
    "{hyb_docs}\n",
    "\n",
    "=== CRITÈRES D’ÉVALUATION ===\n",
    "1. Pertinence globale\n",
    "2. Couverture de l’intention\n",
    "3. Précision du top 3\n",
    "4. Absence de résultats hors sujet\n",
    "\n",
    "=== FORMAT DE SORTIE OBLIGATOIRE (JSON STRICT) ===\n",
    "Retourne uniquement un JSON valide, sans texte supplémentaire :\n",
    "{{\n",
    "  \"BM25\": {{ \"score\": <float 0-5>, \"justification\": \"...\" }},\n",
    "  \"SEMANTIQUE\": {{ \"score\": <float 0-5>, \"justification\": \"...\" }},\n",
    "  \"HYBRIDE\": {{ \"score\": <float 0-5>, \"justification\": \"...\" }},\n",
    "  \"winner\": \"BM25 | SEMANTIQUE | HYBRIDE | TIE\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def format_docs(results):\n",
    "    \"\"\"Helper to format list of recipes into string for LLM\"\"\"\n",
    "    if not results: return \"Aucun résultat.\"\n",
    "    return \"\\n\".join([f\"{i+1}. {r.get('NAME', 'Unknown')} (Desc: {r.get('DESCRIPTION', '')[:100]}...)\" for i, r in enumerate(results)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ca90a-b358-4334-abac-b39f159e11dc",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "from snowflake.cortex import Complete\n",
    "\n",
    "eval_metrics = []\n",
    "\n",
    "print(\"Running Cortex Evaluation (LLM-as-a-Judge)...\")\n",
    "\n",
    "for row in results_data:\n",
    "    # Prepare Prompt\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        query_text=row['query'],\n",
    "        bm25_docs=format_docs(row['bm25_res']),\n",
    "        sem_docs=format_docs(row['sem_res']),\n",
    "        hyb_docs=format_docs(row['hyb_res'])\n",
    "    )\n",
    "    \n",
    "    # Call Cortex\n",
    "    try:\n",
    "        response_str = Complete(JUDGE_MODEL, prompt)\n",
    "        # Simple cleanup in case LLM adds markdown blocks\n",
    "        clean_json = response_str.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        eval_json = json.loads(clean_json)\n",
    "        \n",
    "        eval_metrics.append({\n",
    "            \"query\": row['query'],\n",
    "            \"bm25_score\": eval_json['BM25']['score'],\n",
    "            \"sem_score\": eval_json['SEMANTIQUE']['score'],\n",
    "            \"hyb_score\": eval_json['HYBRIDE']['score'],\n",
    "            \"winner\": eval_json['winner']\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating query '{row['query']}': {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_eval = pd.DataFrame(eval_metrics)\n",
    "df_time = pd.DataFrame([{\n",
    "    \"query\": r['query'], \n",
    "    \"bm25_time\": r['bm25_time'], \n",
    "    \"sem_time\": r['sem_time'], \n",
    "    \"hyb_time\": r['hyb_time']\n",
    "} for r in results_data])\n",
    "\n",
    "# Merge\n",
    "final_df = pd.merge(df_time, df_eval, on=\"query\")\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3581c-38ea-4bb2-a760-ec9718811a76",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "# Reshape for plotting\n",
    "df_melted_time = final_df.melt(id_vars=[\"query\"], value_vars=[\"bm25_time\", \"sem_time\", \"hyb_time\"], \n",
    "                               var_name=\"Method\", value_name=\"Time (ms)\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_melted_time, x=\"query\", y=\"Time (ms)\", hue=\"Method\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Execution Time by Method\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a66b4-b4d9-4623-9bfb-3385860426fc",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "# Reshape for plotting\n",
    "df_melted_score = final_df.melt(id_vars=[\"query\"], value_vars=[\"bm25_score\", \"sem_score\", \"hyb_score\"], \n",
    "                                var_name=\"Method\", value_name=\"Quality Score (0-5)\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_melted_score, x=\"query\", y=\"Quality Score (0-5)\", hue=\"Method\", palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"LLM Quality Evaluation Scores\")\n",
    "plt.ylim(0, 5.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774287d1-b361-4abd-948f-4ba415e6bb6f",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "# --- 1. Strategic Trade-off Scatter Plot ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Calculate Averages\n",
    "avg_data = final_df.mean(numeric_only=True)\n",
    "methods = [('BM25', 'bm25'), ('Semantic', 'sem'), ('Hybrid', 'hyb')]\n",
    "\n",
    "# Plot points\n",
    "for label, key in methods:\n",
    "    score = avg_data[f'{key}_score']\n",
    "    time_val = avg_data[f'{key}_time']\n",
    "    plt.scatter(time_val, score, s=300, label=label)\n",
    "    # Label with Score\n",
    "    plt.text(time_val+1000, score, \n",
    "             f\"  {label}\\n  (Score: {score:.2f})\", \n",
    "             va='center', fontweight='bold')\n",
    "\n",
    "# Draw arrows to show the \"Upgrade Path\"\n",
    "plt.plot([avg_data['bm25_time'], avg_data['sem_time'], avg_data['hyb_time']], \n",
    "         [avg_data['bm25_score'], avg_data['sem_score'], avg_data['hyb_score']], \n",
    "         '--', color='gray', alpha=0.5)\n",
    "\n",
    "plt.title(\"Strategic Trade-off: Latency vs. Quality\")\n",
    "plt.xlabel(\"Latency (ms)\")\n",
    "plt.ylabel(\"Quality Score (0-5)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# --- 2. Net Improvement Calculation ---\n",
    "final_df['delta_vs_sem'] = final_df['hyb_score'] - final_df['sem_score']\n",
    "final_df['delta_vs_bm25'] = final_df['hyb_score'] - final_df['bm25_score']\n",
    "\n",
    "print(\"\\n=== NET IMPROVEMENT ANALYSIS ===\")\n",
    "print(f\"Average Improvement (Hybrid vs Semantic): +{final_df['delta_vs_sem'].mean():.2f} points\")\n",
    "print(f\"Average Improvement (Hybrid vs BM25):     +{final_df['delta_vs_bm25'].mean():.2f} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4444cd0-c79d-45ec-ac3f-ab38a708f8fa",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "summary = final_df.agg({\n",
    "    'bm25_time': 'mean',\n",
    "    'sem_time': 'mean',\n",
    "    'hyb_time': 'mean',\n",
    "    'bm25_score': 'mean',\n",
    "    'sem_score': 'mean',\n",
    "    'hyb_score': 'mean'\n",
    "}).transpose()\n",
    "\n",
    "print(\"=== AVERAGE METRICS ===\")\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n=== WINNER DISTRIBUTION ===\")\n",
    "print(final_df['winner'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a352c-d2b7-4beb-8140-05cca3476187",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "def format_docs(results):\n",
    "    \"\"\"\n",
    "    Transforme la liste JSON des recettes en un texte lisible pour le LLM.\n",
    "    Gère le cas où la liste est vide.\n",
    "    \"\"\"\n",
    "    if not results or len(results) == 0:\n",
    "        return \"Aucun résultat trouvé.\"\n",
    "    \n",
    "    formatted_text = \"\"\n",
    "    for i, r in enumerate(results[:5]): # On limite aux 5 premiers pour ne pas saturer le contexte\n",
    "        name = r.get('NAME', 'Recette Inconnue')\n",
    "        # On coupe la description pour économiser des tokens\n",
    "        desc = r.get('DESCRIPTION', 'Pas de description')[:150].replace(\"\\n\", \" \")\n",
    "        formatted_text += f\"{i+1}. {name} (Desc: {desc}...)\\n\"\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Tu es un expert impartial en évaluation de moteurs de recherche (Information Retrieval).\n",
    "Ta mission est de noter la qualité des résultats pour une requête culinaire donnée.\n",
    "\n",
    "=== REQUÊTE UTILISATEUR ===\n",
    "\"{query_text}\"\n",
    "\n",
    "=== RÉSULTATS À ÉVALUER (HYBRIDE) ===\n",
    "{hyb_docs}\n",
    "\n",
    "=== MÉTRIQUES À ÉVALUER ===\n",
    "1. TOPICAL RELEVANCE (Pertinence Thématique) [0.0 - 5.0] :\n",
    "   - Les documents parlent-ils bien du sujet demandé ? (Ex: Si on cherche \"Gâteau\", est-ce un gâteau ?)\n",
    "   - C'est le \"matching\" basique des mots-clés et du sujet principal.\n",
    "\n",
    "2. INTENT & CONSTRAINT ADHERENCE (Respect de l'Intention & Contraintes) [0.0 - 5.0] :\n",
    "   - Le résultat répond-il au BESOIN PROFOND ou aux CONTRAINTES de l'utilisateur ?\n",
    "   - Cas Contrainte Stricte : Pour \"Gluten free\", si un résultat contient du blé -> Score 1.0 (Pénalité maximale).\n",
    "   - Cas Abstrait/Vibe : Pour \"Comfort food\" ou \"Impress my boss\", le plat est-il psychologiquement adapté ?\n",
    "   - Cas Fonctionnel : Pour \"Easy to digest when sick\", un burger est pertinent thématiquement (c'est de la nourriture) mais a un score d'Intention de 0.\n",
    "\n",
    "3. SCORE : Moyenne pondérée (Donner plus de poids à l'Intention si la requête est spécifique).\n",
    "\n",
    "=== FORMAT DE SORTIE (JSON STRICT) ===\n",
    "Réponds UNIQUEMENT avec ce JSON valide :\n",
    "{{\n",
    "  \"HYBRIDE\": {{\n",
    "    \"relevance\": <float 0-5>,\n",
    "    \"intent_adherence\": <float 0-5>,\n",
    "    \"score\": <float 0-5>,\n",
    "    \"justification\": \"<analyse concise en 1 phrase>\"\n",
    "  }}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1b49e-ba67-41c2-9fb0-2667913fab45",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "weight_candidates = [\n",
    "    (0.3, 0.7),\n",
    "    (0.4, 0.6),\n",
    "    (0.6, 0.4),\n",
    "    (0.7, 0.3),\n",
    "    (0.8, 0.2),\n",
    "]\n",
    "\n",
    "filters_dict = {\n",
    "  \"numeric_filters\": [\n",
    "        {\"name\": \"minutes\", \"operator\": \"<=\", \"value\": 50},\n",
    "        {\"name\": \"servings\", \"operator\": \">=\", \"value\": 2}\n",
    "    ]\n",
    "}\n",
    "filters = json.dumps(filters_dict)\n",
    "\n",
    "tuning_results = []\n",
    "\n",
    "print(f\"=== DÉMARRAGE ({len(weight_candidates)} configs) ===\\n\")\n",
    "\n",
    "for vec_w, bm25_w in weight_candidates:\n",
    "    print(f\"\\n>> CONFIG: Vector={vec_w} / BM25={bm25_w}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Stockage temporaire pour les moyennes\n",
    "    temp_relevance = []\n",
    "    temp_intent_adherence = []\n",
    "    temp_global = []\n",
    "    \n",
    "    for q in test_queries:\n",
    "        try:\n",
    "            # A. RECHERCHE\n",
    "            res_hyb = session.call(\n",
    "                PROC_HYBRID, q, TOP_K, filters, vec_w, bm25_w, \n",
    "                BM25_INDEX, SOURCE_TABLE, EMBEDDINGS_TABLE\n",
    "            )\n",
    "            \n",
    "            if not res_hyb:\n",
    "                continue\n",
    "\n",
    "            # Parsing sécurisé\n",
    "            res_json = json.loads(res_hyb) if isinstance(res_hyb, str) else res_hyb\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   [CRASH SEARCH] '{q}': {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # B. ÉVALUATION CORTEX\n",
    "            prompt = PROMPT_TEMPLATE.format(query_text=q, hyb_docs=format_docs(res_json))\n",
    "            \n",
    "            cmd = \"SELECT snowflake.cortex.COMPLETE(?, ?)\"\n",
    "            cortex_res = session.sql(cmd, params=[JUDGE_MODEL, prompt]).collect()[0][0]\n",
    "            \n",
    "            clean_json = cortex_res.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            eval_data = json.loads(clean_json)\n",
    "            \n",
    "            # Extraction des métriques\n",
    "            metrics = eval_data['HYBRIDE']\n",
    "            \n",
    "            # Affichage pour débogage\n",
    "            print(f\"   Query: {q[:30]:<30} | Rel: {metrics['relevance']} | Faith: {metrics['intent_adherence']} | Global: {metrics['score']}\")\n",
    "            print(f\"{metrics['justification']}\")\n",
    "            \n",
    "            temp_relevance.append(metrics['relevance'])\n",
    "            temp_intent_adherence.append(metrics['intent_adherence'])\n",
    "            temp_global.append(metrics['score'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   [CRASH LLM] '{q}': {e}\")\n",
    "            continue\n",
    "\n",
    "    # Calcul des moyennes pour cette config\n",
    "    if temp_global:\n",
    "        res = {\n",
    "            \"vector_weight\": vec_w,\n",
    "            \"avg_relevance\": sum(temp_relevance) / len(temp_relevance),\n",
    "            \"avg_intent_adherence\": sum(temp_intent_adherence) / len(temp_intent_adherence),\n",
    "            \"avg_global\": sum(temp_global) / len(temp_global)\n",
    "        }\n",
    "        tuning_results.append(res)\n",
    "        print(f\"   >>> MOYENNE CONFIG: Relevance={res['avg_relevance']:.2f} | intent_adherence={res['avg_intent_adherence']:.2f}\")\n",
    "    else:\n",
    "        print(\"   >>> Pas de données valides.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b906e-d22b-4550-8858-320c794988d0",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "# 1. Création du DataFrame à partir des résultats\n",
    "df_tuning = pd.DataFrame(tuning_results)\n",
    "\n",
    "# 2. Transformation des données pour le tracé (Format \"Long\")\n",
    "# Cela permet à Seaborn de tracer 3 lignes distinctes automatiquement\n",
    "df_melted = df_tuning.melt(\n",
    "    id_vars=[\"vector_weight\"], \n",
    "    value_vars=[\"avg_relevance\", \"avg_intent_adherence\", \"avg_global\"],\n",
    "    var_name=\"Type de Score\", \n",
    "    value_name=\"Note (0-5)\"\n",
    ")\n",
    "\n",
    "# Renommage pour la légende (plus propre)\n",
    "metric_map = {\n",
    "    \"avg_relevance\": \"Pertinence (Sujet)\",\n",
    "    \"avg_intent_adherence\": \"Fidélité (Contraintes)\",\n",
    "    \"avg_global\": \"Score Global\"\n",
    "}\n",
    "df_melted[\"Type de Score\"] = df_melted[\"Type de Score\"].map(metric_map)\n",
    "\n",
    "# 3. Configuration du Graphique\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.set_style(\"whitegrid\") # Fond blanc quadrillé plus pro\n",
    "\n",
    "# Tracé des lignes\n",
    "# style=\"Type de Score\" ajoute des pointillés différents pour chaque ligne\n",
    "plot = sns.lineplot(\n",
    "    data=df_melted, \n",
    "    x=\"vector_weight\", \n",
    "    y=\"Note (0-5)\", \n",
    "    hue=\"Type de Score\", \n",
    "    style=\"Type de Score\",\n",
    "    markers=True, \n",
    "    dashes=False, \n",
    "    markersize=10, \n",
    "    linewidth=3\n",
    ")\n",
    "\n",
    "# 4. Esthétique\n",
    "plt.title(\"Analyse Multi-Critères : L'Impact du Poids Vectoriel\", fontsize=16, pad=20)\n",
    "plt.xlabel(\"Poids Vectoriel (Vers la droite = Plus Sémantique)\", fontsize=12)\n",
    "plt.ylabel(\"Score Moyen (0-5)\", fontsize=12)\n",
    "plt.ylim(2.5, 5.2) # On zoom sur la partie intéressante\n",
    "\n",
    "# 5. Annotation du Meilleur Score Global\n",
    "best_row = df_tuning.loc[df_tuning['avg_global'].idxmax()]\n",
    "best_vec = best_row['vector_weight']\n",
    "best_score = best_row['avg_global']\n",
    "\n",
    "plt.annotate(f\"MEILLEUR COMPROMIS\\nGlobal: {best_score:.2f}/5\\n(Vec: {best_vec})\", \n",
    "             xy=(best_vec, best_score), \n",
    "             xytext=(best_vec, best_score + 0.3), \n",
    "             ha='center',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", alpha=0.8),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.legend(title=\"Métriques\", loc='lower center', bbox_to_anchor=(0.5, -0.2), ncol=3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Affichage du tableau de données pour le rapport\n",
    "print(\"\\n=== TABLEAU DE SYNTHÈSE ===\")\n",
    "print(df_tuning.round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "",
   "authorId": "8704862111977",
   "authorName": "DRAGON",
   "lastEditTime": 1767538264155,
   "notebookId": "6djxiwivvypep44mlur4",
   "sessionId": "c4214477-ae8e-4f7a-9dbd-cd72e67802c7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
