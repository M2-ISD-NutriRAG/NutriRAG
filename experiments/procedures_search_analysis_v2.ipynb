{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "6djxiwivvypep44mlur4",
   "authorId": "8704862111977",
   "authorName": "DRAGON",
   "authorEmail": "",
   "sessionId": "c4214477-ae8e-4f7a-9dbd-cd72e67802c7",
   "lastEditTime": 1767538264155
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\n\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8efe0fc6-5d29-42f8-b990-318b45bde4ad",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import snowflake.snowpark.functions as F\nfrom snowflake.snowpark.types import StringType, StructType, StructField\nimport pandas as pd\nimport json\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# === CONFIGURATION ===\n# Table & Index Names (Based on your docs)\nSOURCE_TABLE = \"ENRICHED.RECIPES_SAMPLE_50K\"\nBM25_INDEX = \"VECTORS.RECIPES_SAMPLE_50K_BM25_INDEX\"\nEMBEDDINGS_TABLE = \"VECTORS.RECIPES_50K_EMBEDDINGS\"\n\n# Procedure Names\nPROC_BM25 = \"NUTRIRAG_PROJECT.VECTORS.SEARCH_BM25\"\nPROC_SEMANTIC = \"NUTRIRAG_PROJECT.VECTORS.SEARCH_SEMANTIC\"\nPROC_HYBRID = \"NUTRIRAG_PROJECT.VECTORS.SEARCH_SIMILAR_RECIPES\"\n\n# Hybrid Weights\nVECTOR_WEIGHT = 0.7\nBM25_WEIGHT = 0.3\n\n# Cortex Model for Evaluation (LLM Judge)\nJUDGE_MODEL = \"llama3.1-70b\" # or 'mistral-large' <- trés couteux ce truc ptn\n\n# Test Parameters\nTOP_K = 5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb3715ff-9e3a-4832-ae66-12f2aeba5930",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "test_queries = [\n    \"chocolate cake\",                     # Keyword heavy\n    \"healthy vegetarian dinner quick\",    # Semantic + Constraint\n    \"gluten free pasta\",   \n    \"refreshing summer dessert lemon\",    # Semantic / Vibe\n    \"traditional italian lasagna\",        # Specific entity\n    \"high protein breakfast\",     \n    \"comfort food for rainy day\",         # Purely semantic/abstract\n    \"romantic dinner for two italian style\",  # Vibe + Cuisine\n    \"something easy to digest when sick\",     # Fonctionnel (BM25 va chercher le mot \"sick\"...)\n    \"meal to impress my boss\",                # Social / Abstrait\n    \"post workout high protein recovery\",     # Fonctionnel / Nutrition\n    \"cozy comfort food for winter night\",     # Vibe / Saison\n    \"light lunch that won't make me sleepy\",  # Effet désiré\n    \"kids friendly vegetables dish\"           # Cible\n]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b784874-ed34-4969-bc1d-497e50bd0e59",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def run_bm25(query):\n    \"\"\"Executes BM25 Search Procedure\"\"\"\n    try:\n        # Args: Query, IndexTable, SourceTable, Limit, Filters\n        # Note: Filters are NULL (None) for this benchmark\n        res = session.call(PROC_BM25, query, BM25_INDEX, SOURCE_TABLE, TOP_K, None)\n        return json.loads(res) if isinstance(res, str) else res\n    except Exception as e:\n        print(f\"Error in BM25: {e}\")\n        return []\n\ndef run_semantic(query):\n    \"\"\"Executes Semantic Search Procedure\"\"\"\n    try:\n        # Args: Query, EmbeddingsTable, Limit, Filters\n        res = session.call(PROC_SEMANTIC, query, EMBEDDINGS_TABLE, TOP_K, None)\n        return json.loads(res) if isinstance(res, str) else res\n    except Exception as e:\n        print(f\"Error in Semantic: {e}\")\n        return []\n\ndef run_hybrid(query):\n    \"\"\"Executes Hybrid Search Procedure\"\"\"\n    try:\n        # Args: Query, Limit, Filters, VecWeight, BM25Weight, Index, Source, Embeddings\n        res = session.call(\n            PROC_HYBRID, \n            query, \n            TOP_K, \n            None, \n            VECTOR_WEIGHT, \n            BM25_WEIGHT, \n            BM25_INDEX, \n            SOURCE_TABLE, \n            EMBEDDINGS_TABLE\n        )\n        return json.loads(res) if isinstance(res, str) else res\n    except Exception as e:\n        print(f\"Error in Hybrid: {e}\")\n        return []",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c90910f1-dbeb-4115-a14f-610e915f39e6",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "results_data = []\n\nprint(f\"Starting benchmark for {len(test_queries)} queries...\")\n\nfor q in test_queries:\n    # 1. BM25\n    start = time.time()\n    res_bm25 = run_bm25(q)\n    dur_bm25 = (time.time() - start) * 1000 # ms\n    \n    # 2. Semantic\n    start = time.time()\n    res_sem = run_semantic(q)\n    dur_sem = (time.time() - start) * 1000 # ms\n    \n    # 3. Hybrid\n    start = time.time()\n    res_hyb = run_hybrid(q)\n    dur_hyb = (time.time() - start) * 1000 # ms\n    \n    # Store raw results for the LLM Judge\n    results_data.append({\n        \"query\": q,\n        \"bm25_res\": res_bm25,\n        \"bm25_time\": dur_bm25,\n        \"sem_res\": res_sem,\n        \"sem_time\": dur_sem,\n        \"hyb_res\": res_hyb,\n        \"hyb_time\": dur_hyb\n    })\n\nprint(\"Benchmark execution complete.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aae69195-a747-4063-b558-92860347011e",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "for i, row in enumerate(results_data):\n    print(f\"\\n==================================================\")\n    print(f\"QUERY {i+1}: '{row['query']}'\")\n    print(f\"==================================================\")\n    \n    # Extract just the top 3 recipe names for clarity\n    bm25_names = [r.get('NAME', 'Unknown') for r in row['bm25_res'][:3]]\n    sem_names = [r.get('NAME', 'Unknown') for r in row['sem_res'][:3]]\n    hyb_names = [r.get('NAME', 'Unknown') for r in row['hyb_res'][:3]]\n    \n    # Print comparison\n    print(f\"--- BM25 Found: ---\")\n    for name in bm25_names: print(f\"  • {name}\")\n        \n    print(f\"\\n--- SEMANTIC Found: ---\")\n    for name in sem_names: print(f\"  • {name}\")\n        \n    print(f\"\\n--- HYBRID Found: ---\")\n    for name in hyb_names: print(f\"  • {name}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4e45423-eb89-40b6-9cbb-66789a8499a1",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "\nPROMPT_TEMPLATE = \"\"\"\nTu es un expert en recherche d’information (Information Retrieval) et en évaluation de moteurs de recherche.\nTon objectif est d’évaluer objectivement la qualité des résultats fournis par trois systèmes différents pour une même requête utilisateur.\n\n=== REQUÊTE UTILISATEUR ===\n{query_text}\n\n=== RÉSULTATS DU SYSTÈME A (BM25) ===\n{bm25_docs}\n\n=== RÉSULTATS DU SYSTÈME B (SEMANTIQUE) ===\n{sem_docs}\n\n=== RÉSULTATS DU SYSTÈME C (HYBRIDE) ===\n{hyb_docs}\n\n=== CRITÈRES D’ÉVALUATION ===\n1. Pertinence globale\n2. Couverture de l’intention\n3. Précision du top 3\n4. Absence de résultats hors sujet\n\n=== FORMAT DE SORTIE OBLIGATOIRE (JSON STRICT) ===\nRetourne uniquement un JSON valide, sans texte supplémentaire :\n{{\n  \"BM25\": {{ \"score\": <float 0-5>, \"justification\": \"...\" }},\n  \"SEMANTIQUE\": {{ \"score\": <float 0-5>, \"justification\": \"...\" }},\n  \"HYBRIDE\": {{ \"score\": <float 0-5>, \"justification\": \"...\" }},\n  \"winner\": \"BM25 | SEMANTIQUE | HYBRIDE | TIE\"\n}}\n\"\"\"\n\ndef format_docs(results):\n    \"\"\"Helper to format list of recipes into string for LLM\"\"\"\n    if not results: return \"Aucun résultat.\"\n    return \"\\n\".join([f\"{i+1}. {r.get('NAME', 'Unknown')} (Desc: {r.get('DESCRIPTION', '')[:100]}...)\" for i, r in enumerate(results)])\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "370ca90a-b358-4334-abac-b39f159e11dc",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.cortex import Complete\n\neval_metrics = []\n\nprint(\"Running Cortex Evaluation (LLM-as-a-Judge)...\")\n\nfor row in results_data:\n    # Prepare Prompt\n    prompt = PROMPT_TEMPLATE.format(\n        query_text=row['query'],\n        bm25_docs=format_docs(row['bm25_res']),\n        sem_docs=format_docs(row['sem_res']),\n        hyb_docs=format_docs(row['hyb_res'])\n    )\n    \n    # Call Cortex\n    try:\n        response_str = Complete(JUDGE_MODEL, prompt)\n        # Simple cleanup in case LLM adds markdown blocks\n        clean_json = response_str.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n        eval_json = json.loads(clean_json)\n        \n        eval_metrics.append({\n            \"query\": row['query'],\n            \"bm25_score\": eval_json['BM25']['score'],\n            \"sem_score\": eval_json['SEMANTIQUE']['score'],\n            \"hyb_score\": eval_json['HYBRIDE']['score'],\n            \"winner\": eval_json['winner']\n        })\n    except Exception as e:\n        print(f\"Error evaluating query '{row['query']}': {e}\")\n\n# Create DataFrame\ndf_eval = pd.DataFrame(eval_metrics)\ndf_time = pd.DataFrame([{\n    \"query\": r['query'], \n    \"bm25_time\": r['bm25_time'], \n    \"sem_time\": r['sem_time'], \n    \"hyb_time\": r['hyb_time']\n} for r in results_data])\n\n# Merge\nfinal_df = pd.merge(df_time, df_eval, on=\"query\")\nfinal_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8a3581c-38ea-4bb2-a760-ec9718811a76",
   "metadata": {
    "language": "python",
    "name": "cell8",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Reshape for plotting\ndf_melted_time = final_df.melt(id_vars=[\"query\"], value_vars=[\"bm25_time\", \"sem_time\", \"hyb_time\"], \n                               var_name=\"Method\", value_name=\"Time (ms)\")\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=df_melted_time, x=\"query\", y=\"Time (ms)\", hue=\"Method\")\nplt.xticks(rotation=45, ha='right')\nplt.title(\"Execution Time by Method\")\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ba3a66b4-b4d9-4623-9bfb-3385860426fc",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Reshape for plotting\ndf_melted_score = final_df.melt(id_vars=[\"query\"], value_vars=[\"bm25_score\", \"sem_score\", \"hyb_score\"], \n                                var_name=\"Method\", value_name=\"Quality Score (0-5)\")\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=df_melted_score, x=\"query\", y=\"Quality Score (0-5)\", hue=\"Method\", palette=\"viridis\")\nplt.xticks(rotation=45, ha='right')\nplt.title(\"LLM Quality Evaluation Scores\")\nplt.ylim(0, 5.5)\nplt.tight_layout()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "774287d1-b361-4abd-948f-4ba415e6bb6f",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "# --- 1. Strategic Trade-off Scatter Plot ---\nplt.figure(figsize=(8, 6))\n\n# Calculate Averages\navg_data = final_df.mean(numeric_only=True)\nmethods = [('BM25', 'bm25'), ('Semantic', 'sem'), ('Hybrid', 'hyb')]\n\n# Plot points\nfor label, key in methods:\n    score = avg_data[f'{key}_score']\n    time_val = avg_data[f'{key}_time']\n    plt.scatter(time_val, score, s=300, label=label)\n    # Label with Score\n    plt.text(time_val+1000, score, \n             f\"  {label}\\n  (Score: {score:.2f})\", \n             va='center', fontweight='bold')\n\n# Draw arrows to show the \"Upgrade Path\"\nplt.plot([avg_data['bm25_time'], avg_data['sem_time'], avg_data['hyb_time']], \n         [avg_data['bm25_score'], avg_data['sem_score'], avg_data['hyb_score']], \n         '--', color='gray', alpha=0.5)\n\nplt.title(\"Strategic Trade-off: Latency vs. Quality\")\nplt.xlabel(\"Latency (ms)\")\nplt.ylabel(\"Quality Score (0-5)\")\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n# --- 2. Net Improvement Calculation ---\nfinal_df['delta_vs_sem'] = final_df['hyb_score'] - final_df['sem_score']\nfinal_df['delta_vs_bm25'] = final_df['hyb_score'] - final_df['bm25_score']\n\nprint(\"\\n=== NET IMPROVEMENT ANALYSIS ===\")\nprint(f\"Average Improvement (Hybrid vs Semantic): +{final_df['delta_vs_sem'].mean():.2f} points\")\nprint(f\"Average Improvement (Hybrid vs BM25):     +{final_df['delta_vs_bm25'].mean():.2f} points\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d4444cd0-c79d-45ec-ac3f-ab38a708f8fa",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "summary = final_df.agg({\n    'bm25_time': 'mean',\n    'sem_time': 'mean',\n    'hyb_time': 'mean',\n    'bm25_score': 'mean',\n    'sem_score': 'mean',\n    'hyb_score': 'mean'\n}).transpose()\n\nprint(\"=== AVERAGE METRICS ===\")\nprint(summary)\n\nprint(\"\\n=== WINNER DISTRIBUTION ===\")\nprint(final_df['winner'].value_counts())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "477a352c-d2b7-4beb-8140-05cca3476187",
   "metadata": {
    "language": "python",
    "name": "cell15",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def format_docs(results):\n    \"\"\"\n    Transforme la liste JSON des recettes en un texte lisible pour le LLM.\n    Gère le cas où la liste est vide.\n    \"\"\"\n    if not results or len(results) == 0:\n        return \"Aucun résultat trouvé.\"\n    \n    formatted_text = \"\"\n    for i, r in enumerate(results[:5]): # On limite aux 5 premiers pour ne pas saturer le contexte\n        name = r.get('NAME', 'Recette Inconnue')\n        # On coupe la description pour économiser des tokens\n        desc = r.get('DESCRIPTION', 'Pas de description')[:150].replace(\"\\n\", \" \")\n        formatted_text += f\"{i+1}. {name} (Desc: {desc}...)\\n\"\n    \n    return formatted_text\n\nPROMPT_TEMPLATE = \"\"\"\nTu es un expert impartial en évaluation de moteurs de recherche (Information Retrieval).\nTa mission est de noter la qualité des résultats pour une requête culinaire donnée.\n\n=== REQUÊTE UTILISATEUR ===\n\"{query_text}\"\n\n=== RÉSULTATS À ÉVALUER (HYBRIDE) ===\n{hyb_docs}\n\n=== MÉTRIQUES À ÉVALUER ===\n1. RELEVANCE (Pertinence Sémantique) [0.0 - 5.0] :\n   - Les documents trouvés correspondent-ils thématiquement à la recherche ?\n   - Ignore les contraintes de détail ici, juge le sujet global (Ex: Si on cherche \"Lasagne\", est-ce des lasagnes ?).\n\n2. FAITHFULNESS (Respect des Contraintes) [0.0 - 5.0] :\n   - Le système a-t-il respecté les instructions spécifiques (intolérances, ingrédients exclus, type de plat) ?\n   - C'est une mesure de précision binaire : Si une recette contient du gluten alors qu'on demande \"sans gluten\", ce score doit être très bas (1.0).\n\n3. SCORE GLOBAL : Moyenne pondérée des deux précédents.\n\n=== FORMAT DE SORTIE (JSON STRICT) ===\nRéponds UNIQUEMENT avec ce JSON valide :\n{{\n  \"HYBRIDE\": {{\n    \"relevance\": <float 0-5>,\n    \"faithfulness\": <float 0-5>,\n    \"score\": <float 0-5>,\n    \"justification\": \"<analyse concise en 1 phrase>\"\n  }}\n}}\n\"\"\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9a1b49e-ba67-41c2-9fb0-2667913fab45",
   "metadata": {
    "language": "python",
    "name": "cell12",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "weight_candidates = [\n    (0.3, 0.7),\n    (0.4, 0.6),\n    (0.6, 0.4),\n    (0.7, 0.3),\n    (0.8, 0.2),\n]\n\nfilters_dict = {\n  \"numeric_filters\": [\n        {\"name\": \"minutes\", \"operator\": \"<=\", \"value\": 50},\n        {\"name\": \"servings\", \"operator\": \">=\", \"value\": 2}\n    ]\n}\nfilters = json.dumps(filters_dict)\n\ntuning_results = []\n\nprint(f\"=== DÉMARRAGE ({len(weight_candidates)} configs) ===\\n\")\n\nfor vec_w, bm25_w in weight_candidates:\n    print(f\"\\n>> CONFIG: Vector={vec_w} / BM25={bm25_w}\")\n    print(\"-\" * 60)\n    \n    # Stockage temporaire pour les moyennes\n    temp_relevance = []\n    temp_faithfulness = []\n    temp_global = []\n    \n    for q in test_queries:\n        try:\n            # A. RECHERCHE\n            res_hyb = session.call(\n                PROC_HYBRID, q, TOP_K, filters, vec_w, bm25_w, \n                BM25_INDEX, SOURCE_TABLE, EMBEDDINGS_TABLE\n            )\n            \n            if not res_hyb:\n                continue\n\n            # Parsing sécurisé\n            res_json = json.loads(res_hyb) if isinstance(res_hyb, str) else res_hyb\n                \n        except Exception as e:\n            print(f\"   [CRASH SEARCH] '{q}': {e}\")\n            continue\n\n        try:\n            # B. ÉVALUATION CORTEX\n            prompt = PROMPT_TEMPLATE.format(query_text=q, hyb_docs=format_docs(res_json))\n            \n            cmd = \"SELECT snowflake.cortex.COMPLETE(?, ?)\"\n            cortex_res = session.sql(cmd, params=[JUDGE_MODEL, prompt]).collect()[0][0]\n            \n            clean_json = cortex_res.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n            eval_data = json.loads(clean_json)\n            \n            # Extraction des métriques\n            metrics = eval_data['HYBRIDE']\n            \n            # Affichage pour débogage\n            print(f\"   Query: {q[:30]:<30} | Rel: {metrics['relevance']} | Faith: {metrics['faithfulness']} | Global: {metrics['score']}\")\n            \n            temp_relevance.append(metrics['relevance'])\n            temp_faithfulness.append(metrics['faithfulness'])\n            temp_global.append(metrics['score'])\n            \n        except Exception as e:\n            print(f\"   [CRASH LLM] '{q}': {e}\")\n            continue\n\n    # Calcul des moyennes pour cette config\n    if temp_global:\n        res = {\n            \"vector_weight\": vec_w,\n            \"avg_relevance\": sum(temp_relevance) / len(temp_relevance),\n            \"avg_faithfulness\": sum(temp_faithfulness) / len(temp_faithfulness),\n            \"avg_global\": sum(temp_global) / len(temp_global)\n        }\n        tuning_results.append(res)\n        print(f\"   >>> MOYENNE CONFIG: Relevance={res['avg_relevance']:.2f} | Faithfulness={res['avg_faithfulness']:.2f}\")\n    else:\n        print(\"   >>> Pas de données valides.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eb7b906e-d22b-4550-8858-320c794988d0",
   "metadata": {
    "language": "python",
    "name": "cell17",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# 1. Création du DataFrame à partir des résultats\ndf_tuning = pd.DataFrame(tuning_results)\n\n# 2. Transformation des données pour le tracé (Format \"Long\")\n# Cela permet à Seaborn de tracer 3 lignes distinctes automatiquement\ndf_melted = df_tuning.melt(\n    id_vars=[\"vector_weight\"], \n    value_vars=[\"avg_relevance\", \"avg_faithfulness\", \"avg_global\"],\n    var_name=\"Type de Score\", \n    value_name=\"Note (0-5)\"\n)\n\n# Renommage pour la légende (plus propre)\nmetric_map = {\n    \"avg_relevance\": \"Pertinence (Sujet)\",\n    \"avg_faithfulness\": \"Fidélité (Contraintes)\",\n    \"avg_global\": \"Score Global\"\n}\ndf_melted[\"Type de Score\"] = df_melted[\"Type de Score\"].map(metric_map)\n\n# 3. Configuration du Graphique\nplt.figure(figsize=(12, 7))\nsns.set_style(\"whitegrid\") # Fond blanc quadrillé plus pro\n\n# Tracé des lignes\n# style=\"Type de Score\" ajoute des pointillés différents pour chaque ligne\nplot = sns.lineplot(\n    data=df_melted, \n    x=\"vector_weight\", \n    y=\"Note (0-5)\", \n    hue=\"Type de Score\", \n    style=\"Type de Score\",\n    markers=True, \n    dashes=False, \n    markersize=10, \n    linewidth=3\n)\n\n# 4. Esthétique\nplt.title(\"Analyse Multi-Critères : L'Impact du Poids Vectoriel\", fontsize=16, pad=20)\nplt.xlabel(\"Poids Vectoriel (Vers la droite = Plus Sémantique)\", fontsize=12)\nplt.ylabel(\"Score Moyen (0-5)\", fontsize=12)\nplt.ylim(2.5, 5.2) # On zoom sur la partie intéressante\n\n# 5. Annotation du Meilleur Score Global\nbest_row = df_tuning.loc[df_tuning['avg_global'].idxmax()]\nbest_vec = best_row['vector_weight']\nbest_score = best_row['avg_global']\n\nplt.annotate(f\"MEILLEUR COMPROMIS\\nGlobal: {best_score:.2f}/5\\n(Vec: {best_vec})\", \n             xy=(best_vec, best_score), \n             xytext=(best_vec, best_score + 0.3), \n             ha='center',\n             bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", alpha=0.8),\n             arrowprops=dict(facecolor='black', shrink=0.05))\n\nplt.legend(title=\"Métriques\", loc='lower center', bbox_to_anchor=(0.5, -0.2), ncol=3)\nplt.tight_layout()\nplt.show()\n\n# 6. Affichage du tableau de données pour le rapport\nprint(\"\\n=== TABLEAU DE SYNTHÈSE ===\")\nprint(df_tuning.round(2))",
   "execution_count": null
  }
 ]
}