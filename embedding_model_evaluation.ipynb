{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31fe797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "import plotly.express as px\n",
    "import time, json, os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca8705",
   "metadata": {},
   "source": [
    "**load the config file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "468a3667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "CONFIG_FILE_PATH = os.getenv(\"CONFIG_FILE_PATH\")\n",
    "\n",
    "with open(CONFIG_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d466bd",
   "metadata": {},
   "source": [
    "**load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24756593",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIENCE_ID = os.getenv(\"EXPERIENCE_ID\")\n",
    "\n",
    "INPUT_EMBEDDINGS_FILE = config[\"output_recipies_embedding_file\"].format(\n",
    "    experiment_id=EXPERIENCE_ID\n",
    ")\n",
    "\n",
    "df_recipes_cleaned = pd.read_csv(INPUT_EMBEDDINGS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91ecb2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "emb_columns = [col for col in df_recipes_cleaned.columns if col.endswith('_EMB')]\n",
    "\n",
    "for col in emb_columns:\n",
    "    df_recipes_cleaned[col] = df_recipes_cleaned[col].apply(\n",
    "        lambda x: np.fromstring(x.strip('[]'), sep=' ')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33960b75",
   "metadata": {},
   "source": [
    "**define clustering algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b11012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = config[\"output_clustering_dir\"].format(\n",
    "    experiment_id=EXPERIENCE_ID\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# UMAP GRID - more configurations\n",
    "# --------------------------\n",
    "UMAP_GRID = [\n",
    "    {\"n_neighbors\": 10, \"min_dist\": 0.1, \"n_components\": 20, \"metric\": \"cosine\"},\n",
    "    # {\"n_neighbors\": 15, \"min_dist\": 0.1, \"n_components\": 20, \"metric\": \"cosine\"},\n",
    "    # {\"n_neighbors\": 30, \"min_dist\": 0.1, \"n_components\": 30, \"metric\": \"cosine\"},\n",
    "    # {\"n_neighbors\": 15, \"min_dist\": 0.3, \"n_components\": 30, \"metric\": \"cosine\"},\n",
    "    # {\"n_neighbors\": 50, \"min_dist\": 0.5, \"n_components\": 50, \"metric\": \"cosine\"},\n",
    "]\n",
    "\n",
    "# --------------------------\n",
    "# Clustering GRID - more diversity\n",
    "# --------------------------\n",
    "CLUSTERING_GRID = [\n",
    "    # KMeans\n",
    "    {\"name\": \"kmeans\", \"params\": {\"n_clusters\": 6}},\n",
    "    {\"name\": \"kmeans\", \"params\": {\"n_clusters\": 8}},\n",
    "    {\"name\": \"kmeans\", \"params\": {\"n_clusters\": 10}},\n",
    "    {\"name\": \"kmeans\", \"params\": {\"n_clusters\": 12}},\n",
    "    {\"name\": \"kmeans\", \"params\": {\"n_clusters\": 15}},\n",
    "\n",
    "    # Agglomerative\n",
    "    {\"name\": \"agglomerative\", \"params\": {\"n_clusters\": 6, \"linkage\": \"average\", \"metric\": \"cosine\"}},\n",
    "    {\"name\": \"agglomerative\", \"params\": {\"n_clusters\": 8, \"linkage\": \"average\", \"metric\": \"cosine\"}},\n",
    "    {\"name\": \"agglomerative\", \"params\": {\"n_clusters\": 10, \"linkage\": \"complete\", \"metric\": \"cosine\"}},\n",
    "    {\"name\": \"agglomerative\", \"params\": {\"n_clusters\": 12, \"linkage\": \"complete\", \"metric\": \"cosine\"}},\n",
    "\n",
    "    # HDBSCAN - density-based\n",
    "    {\"name\": \"hdbscan\", \"params\": {\"min_cluster_size\": 5, \"min_samples\": 1}},\n",
    "    {\"name\": \"hdbscan\", \"params\": {\"min_cluster_size\": 10, \"min_samples\": 1}},\n",
    "    {\"name\": \"hdbscan\", \"params\": {\"min_cluster_size\": 15, \"min_samples\": 5}},\n",
    "    {\"name\": \"hdbscan\", \"params\": {\"min_cluster_size\": 20, \"min_samples\": 5}},\n",
    "\n",
    "    # DBSCAN - optional alternative\n",
    "    {\"name\": \"dbscan\", \"params\": {\"eps\": 0.2, \"min_samples\": 3, \"metric\": \"cosine\"}},\n",
    "    {\"name\": \"dbscan\", \"params\": {\"eps\": 0.3, \"min_samples\": 5, \"metric\": \"cosine\"}},\n",
    "\n",
    "    # Spectral clustering\n",
    "    {\"name\": \"spectral\", \"params\": {\"n_clusters\": 8}},\n",
    "    {\"name\": \"spectral\", \"params\": {\"n_clusters\": 10}},\n",
    "    {\"name\": \"spectral\", \"params\": {\"n_clusters\": 12}},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f8e01",
   "metadata": {},
   "source": [
    "**calculate metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a511a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(emb_for_metric, labels):\n",
    "    res = {}\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    unique_labels = set(labels)\n",
    "    n_clusters = len([l for l in unique_labels if l != -1])\n",
    "\n",
    "    res[\"n_clusters\"] = n_clusters\n",
    "    res[\"n_points\"] = len(labels)\n",
    "\n",
    "    try:\n",
    "        if n_clusters >= 2:\n",
    "            res[\"silhouette\"] = silhouette_score(\n",
    "                emb_for_metric, labels, metric=\"cosine\"\n",
    "            )\n",
    "        else:\n",
    "            res[\"silhouette\"] = float(\"nan\")\n",
    "    except Exception:\n",
    "        res[\"silhouette\"] = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        if n_clusters >= 2:\n",
    "            res[\"davies_bouldin\"] = davies_bouldin_score(emb_for_metric, labels)\n",
    "        else:\n",
    "            res[\"davies_bouldin\"] = float(\"nan\")\n",
    "    except Exception:\n",
    "        res[\"davies_bouldin\"] = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        if n_clusters >= 2:\n",
    "            res[\"calinski_harabasz\"] = calinski_harabasz_score(\n",
    "                emb_for_metric, labels\n",
    "            )\n",
    "        else:\n",
    "            res[\"calinski_harabasz\"] = float(\"nan\")\n",
    "    except Exception:\n",
    "        res[\"calinski_harabasz\"] = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        emb = np.array(emb_for_metric)\n",
    "        cos_sim = cosine_similarity(emb)\n",
    "\n",
    "        # ---- Intra-cluster ----\n",
    "        intra_sims = []\n",
    "        for cl in unique_labels:\n",
    "            if cl == -1:  # skip noise\n",
    "                continue\n",
    "            idx = np.where(labels == cl)[0]\n",
    "            if len(idx) > 1:\n",
    "                sims = cos_sim[np.ix_(idx, idx)]\n",
    "                # exclude self-similarity (diag=1)\n",
    "                sims = sims[np.triu_indices_from(sims, k=1)]\n",
    "                if len(sims) > 0:\n",
    "                    intra_sims.append(np.mean(sims))\n",
    "\n",
    "        res[\"intra_cluster_cosine_mean\"] = (\n",
    "            float(np.mean(intra_sims)) if len(intra_sims) > 0 else float(\"nan\")\n",
    "        )\n",
    "\n",
    "        # ---- Inter-cluster ----\n",
    "        inter_sims = []\n",
    "        clusters = [cl for cl in unique_labels if cl != -1]\n",
    "\n",
    "        for i, c1 in enumerate(clusters):\n",
    "            idx1 = np.where(labels == c1)[0]\n",
    "            for c2 in clusters[i+1:]:\n",
    "                idx2 = np.where(labels == c2)[0]\n",
    "                sims = cos_sim[np.ix_(idx1, idx2)].reshape(-1)\n",
    "                if len(sims) > 0:\n",
    "                    inter_sims.append(np.mean(sims))\n",
    "\n",
    "        res[\"inter_cluster_cosine_mean\"] = (\n",
    "            float(np.mean(inter_sims)) if len(inter_sims) > 0 else float(\"nan\")\n",
    "        )\n",
    "\n",
    "    except Exception:\n",
    "        res[\"intra_cluster_cosine_mean\"] = float(\"nan\")\n",
    "        res[\"inter_cluster_cosine_mean\"] = float(\"nan\")\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "294aa222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clustering_weight(\n",
    "        sil,\n",
    "        db,\n",
    "        ch,\n",
    "        intra,\n",
    "        inter\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate a normalized clustering quality weight from multiple metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sil : float\n",
    "        Silhouette score.\n",
    "    db : float\n",
    "        Davies-Bouldin index (lower is better).\n",
    "    ch : float\n",
    "        Calinski-Harabasz index (higher is better).\n",
    "    intra : float\n",
    "        Intra-cluster cosine similarity (higher is better).\n",
    "    inter : float\n",
    "        Inter-cluster cosine similarity (lower is better).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Normalized weight in range [0, 1].\n",
    "    \"\"\"\n",
    "\n",
    "    # If any metric is missing return a very low weight\n",
    "    if any(np.isnan(x) for x in [sil, db, ch, intra, inter]):\n",
    "        return 0.0\n",
    "\n",
    "    sil_norm = (sil + 1) / 2  \n",
    "\n",
    "    db_norm = 1.0 / (1.0 + db)\n",
    "\n",
    "    ch_norm = np.log1p(max(ch, 0)) / 10\n",
    "    ch_norm = min(ch_norm, 1.0)\n",
    "\n",
    "    # intra-cluster cosine ∈ [-1, 1] → [0, 1]\n",
    "    intra_norm = (intra + 1) / 2  \n",
    "\n",
    "    # inter-cluster cosine: lower is better → invert → [0, 1]\n",
    "    # If inter is near zero or negative → good\n",
    "    inter_norm = 1.0 - ((inter + 1) / 2)\n",
    "    inter_norm = max(min(inter_norm, 1.0), 0.0)\n",
    "\n",
    "    W_SIL = 0.3\n",
    "    W_DBI = 0.1\n",
    "    W_CH  = 0.1\n",
    "    W_INTRA = 0.3\n",
    "    W_INTER = 0.2\n",
    "\n",
    "    # Weighted sum\n",
    "    weight = (\n",
    "        W_SIL   * sil_norm +\n",
    "        W_DBI   * db_norm +\n",
    "        W_CH    * ch_norm +\n",
    "        W_INTRA * intra_norm +\n",
    "        W_INTER * inter_norm\n",
    "    )\n",
    "\n",
    "    # Ensure final weight ∈ [0,1]\n",
    "    return float(min(max(weight, 0.0), 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038f7ed",
   "metadata": {},
   "source": [
    "**execute the clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9ed682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def run_clustering_grid(\n",
    "    emebdding_config,\n",
    "    df_vectors,\n",
    "    umap_grid,\n",
    "    clustering_grid,\n",
    "    output_path=OUT_DIR\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs UMAP + clustering grid search, computes metrics and weights,\n",
    "    and writes the output to JSON.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    emebdding_config : str\n",
    "        Config of embedding model and columns used for embedding\n",
    "    df_vectors : np.ndarray or DataFrame\n",
    "        Your high-dimensional recipe vectors.\n",
    "    umap_grid : list of dict\n",
    "        Grid of UMAP configurations.\n",
    "    clustering_grid : list of dict\n",
    "        Grid of clustering configurations.\n",
    "    output_path : str\n",
    "        Path where results JSON will be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple (all_weights, all_labels)\n",
    "        Tuple containing the weight for each clustering algo and the labels of each one.\n",
    "    \"\"\"\n",
    "\n",
    "    recipe_vectors = np.array(df_vectors)\n",
    "    results_json = []\n",
    "    all_labels = []\n",
    "    all_weights = []\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    for umap_cfg in tqdm(umap_grid, total = len(umap_grid), desc=f\"executing dimension reduction + clustering\"):\n",
    "        reducer = umap.UMAP(\n",
    "            n_neighbors=umap_cfg[\"n_neighbors\"],\n",
    "            min_dist=umap_cfg[\"min_dist\"],\n",
    "            n_components=umap_cfg[\"n_components\"],\n",
    "            metric=umap_cfg.get(\"metric\", \"cosine\"),\n",
    "            random_state=42\n",
    "        )\n",
    "        emb_reduced = reducer.fit_transform(recipe_vectors)\n",
    "\n",
    "        for cl_cfg in clustering_grid:\n",
    "            method = cl_cfg[\"name\"]\n",
    "            params = cl_cfg[\"params\"].copy()\n",
    "            labels = None\n",
    "\n",
    "            # ===== CLUSTERING =====\n",
    "            if method == \"kmeans\":\n",
    "                model = KMeans(n_clusters=params[\"n_clusters\"], random_state=42, n_init=10)\n",
    "                labels = model.fit_predict(emb_reduced)\n",
    "\n",
    "            elif method == \"agglomerative\":\n",
    "                model = AgglomerativeClustering(\n",
    "                    n_clusters=params[\"n_clusters\"],\n",
    "                    metric=params.get(\"metric\", \"cosine\"),\n",
    "                    linkage=params.get(\"linkage\", \"average\")\n",
    "                )\n",
    "                labels = model.fit_predict(emb_reduced)\n",
    "\n",
    "            elif method == \"hdbscan\":\n",
    "                clusterer = hdbscan.HDBSCAN(\n",
    "                    min_cluster_size=params.get(\"min_cluster_size\", 10),\n",
    "                    min_samples=params.get(\"min_samples\", 1),\n",
    "                    metric='euclidean'\n",
    "                )\n",
    "                labels = clusterer.fit_predict(emb_reduced)\n",
    "\n",
    "            elif method == \"dbscan\":\n",
    "                clusterer = DBSCAN(\n",
    "                    eps=params.get(\"eps\", 0.2),\n",
    "                    min_samples=params.get(\"min_samples\", 3),\n",
    "                    metric=params.get(\"metric\", \"cosine\")\n",
    "                )\n",
    "                labels = clusterer.fit_predict(emb_reduced)\n",
    "\n",
    "            elif method == \"spectral\":\n",
    "                model = SpectralClustering(\n",
    "                    n_clusters=params[\"n_clusters\"],\n",
    "                    affinity=\"nearest_neighbors\",\n",
    "                    random_state=42,\n",
    "                    assign_labels=\"kmeans\"\n",
    "                )\n",
    "                labels = model.fit_predict(emb_reduced)\n",
    "\n",
    "            else:\n",
    "                labels = np.array([-1] * len(recipe_vectors))\n",
    "\n",
    "            # ===== METRICS =====\n",
    "            metrics = compute_metrics(emb_reduced, labels)\n",
    "            sil = metrics[\"silhouette\"]\n",
    "            db = metrics[\"davies_bouldin\"]\n",
    "            ch = metrics[\"calinski_harabasz\"]\n",
    "            intra = metrics[\"intra_cluster_cosine_mean\"]\n",
    "            inter = metrics[\"inter_cluster_cosine_mean\"]\n",
    "\n",
    "            # ===== WEIGHT =====\n",
    "            weight = calculate_clustering_weight(sil, db, ch, intra, inter)\n",
    "\n",
    "            # Store valid clustering\n",
    "            if labels is not None and len(np.unique(labels[labels >= 0])) > 1 and weight > 0:\n",
    "                all_labels.append(labels)\n",
    "                all_weights.append(weight)\n",
    "\n",
    "            results_json.append({\n",
    "                \"model_name\": \"recipes_clustering_grid\",\n",
    "                \"umap\": umap_cfg,\n",
    "                \"clustering\": {\"method\": method, \"params\": params},\n",
    "                \"results\": {\n",
    "                    \"n_clusters_found\": metrics[\"n_clusters\"],\n",
    "                    \"silhouette\": sil,\n",
    "                    \"davies_bouldin\": db,\n",
    "                    \"calinski_harabasz\": ch,\n",
    "                    \"intra_cluster_cosine_mean\": intra,\n",
    "                    \"inter_cluster_cosine_mean\": inter,\n",
    "                    \"quality_weight\": float(weight)\n",
    "                }\n",
    "            })\n",
    "\n",
    "    os.makedirs(f\"{output_path}/{emebdding_config}\", exist_ok=True)\n",
    "    with open(f\"{output_path}/{emebdding_config}/clustering.json\", \"w\") as f:\n",
    "        json.dump(results_json, f, indent=2)\n",
    "\n",
    "    return all_weights, all_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3ded0",
   "metadata": {},
   "source": [
    "**execute consencus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d9a5d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def claculate_consencus_stability(\n",
    "    best_consensus_labels,\n",
    "    similarity_matrix\n",
    "):\n",
    "\n",
    "    stability_scores = []\n",
    "    for i in range(len(similarity_matrix)):\n",
    "        my_cluster = best_consensus_labels[i]\n",
    "        \n",
    "        # Find all points in the same consensus cluster\n",
    "        cluster_members = np.where(best_consensus_labels == my_cluster)[0]\n",
    "        \n",
    "        if len(cluster_members) > 1:\n",
    "            # Average co-occurrence with other members of my cluster\n",
    "            cooccur_with_cluster = similarity_matrix[i, cluster_members].sum() - similarity_matrix[i, i]\n",
    "            avg_cooccur = cooccur_with_cluster / (len(cluster_members) - 1)\n",
    "            stability_scores.append(avg_cooccur)\n",
    "        else:\n",
    "            # Singleton cluster\n",
    "            stability_scores.append(0.0)\n",
    "\n",
    "    stability_scores = np.array(stability_scores)\n",
    "    \n",
    "    return stability_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "478749d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concensure_clustering(\n",
    "    emebdding_config,\n",
    "    df_vectors,\n",
    "    all_weights, \n",
    "    all_labels,\n",
    "    output_path=OUT_DIR\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs a clustering on those result to find the best K number of cluster taking \n",
    "    into account the weight of each clustering\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_config : str\n",
    "        the configuration of model and columns used for embedding\n",
    "    df_vectors : np.ndarray or DataFrame\n",
    "        Your high-dimensional recipe vectors.\n",
    "    all_weights : list of weight\n",
    "        Grid of UMAP Weight of the clustering algorithm.\n",
    "    all_labels : list of labels\n",
    "        Labels of point for each clustering algorithm.\n",
    "    output_path : str\n",
    "        Path where results JSON will be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing all run results and consensus result.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = len(df_vectors)\n",
    "    all_weights = np.array(all_weights)\n",
    "    n_clusterings = len(all_labels)\n",
    "\n",
    "    #create the consoncuse matric\n",
    "    co_occur = np.zeros((n_samples, n_samples), dtype=float)\n",
    "\n",
    "    for labels, weight in tqdm(zip(all_labels, all_weights),\n",
    "                               total=n_clusterings, desc=f\"calculating_consensus_matrix\"):\n",
    "        for i in range(n_samples):\n",
    "            if labels[i] == -1:\n",
    "                continue\n",
    "            for j in range(i + 1, n_samples):\n",
    "                if labels[j] == -1:\n",
    "                    continue\n",
    "                if labels[i] == labels[j]:\n",
    "                    co_occur[i, j] += weight\n",
    "                    co_occur[j, i] += weight\n",
    "\n",
    "    similarity_matrix = co_occur / all_weights.sum()\n",
    "    distance_matrix = 1 - similarity_matrix\n",
    "\n",
    "    # Select best k using silhouette score\n",
    "    best_sil = -1\n",
    "    best_k = None\n",
    "    best_consensus_labels = None\n",
    "    max_cluster = 30\n",
    "\n",
    "    for k in tqdm(range(2, min(max_cluster, n_samples)), total=max_cluster, desc=\"calculating_optimal_K\"):\n",
    "        clusterer = AgglomerativeClustering(n_clusters=k, linkage=\"average\")\n",
    "\n",
    "        consensus_labels = clusterer.fit_predict(distance_matrix)\n",
    "        sil = silhouette_score(distance_matrix, consensus_labels)\n",
    "    \n",
    "        if sil > best_sil:\n",
    "            best_sil = sil\n",
    "            best_k = k\n",
    "            best_consensus_labels = consensus_labels\n",
    "\n",
    "    consensus_metrics = compute_metrics(df_vectors, best_consensus_labels)\n",
    "    stability = claculate_consencus_stability(best_consensus_labels, similarity_matrix)\n",
    "    score_clustering = calculate_clustering_weight(consensus_metrics[\"silhouette\"],\n",
    "                                        consensus_metrics[\"davies_bouldin\"],\n",
    "                                        consensus_metrics[\"calinski_harabasz\"],\n",
    "                                        consensus_metrics[\"intra_cluster_cosine_mean\"],\n",
    "                                        consensus_metrics[\"inter_cluster_cosine_mean\"]) + stability * 0.1 \n",
    "\n",
    "    consensus_result = {\n",
    "        \"n_clusters\": best_k,\n",
    "        \"silhouette\": consensus_metrics[\"silhouette\"],\n",
    "        \"davies_bouldin\": consensus_metrics[\"davies_bouldin\"],\n",
    "        \"calinski_harabasz\": consensus_metrics[\"calinski_harabasz\"],\n",
    "        \"intra_cluster_cosine_mean\": consensus_metrics[\"intra_cluster_cosine_mean\"],\n",
    "        \"inter_cluster_cosine_mean\": consensus_metrics[\"inter_cluster_cosine_mean\"],\n",
    "        \"stability\": stability,\n",
    "        \"score_clustering\": score_clustering\n",
    "    }\n",
    "\n",
    "    os.makedirs(f\"{output_path}/{emebdding_config}\", exist_ok=True)\n",
    "\n",
    "    with open(f\"{output_path}/{emebdding_config}/consencus_clustering.json\", \"w\") as f:\n",
    "        json.dump(consensus_result, f, indent=2)\n",
    "\n",
    "    return best_consensus_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dbe035",
   "metadata": {},
   "source": [
    "**calculate metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f3df6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Snowflake/snowflake-arctic-embed-m-v1.5/config_1_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.96s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.64it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Snowflake/snowflake-arctic-embed-m/config_1_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.96s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.48it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____intfloat/e5-base-v2/config_1_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.78s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  8.74it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____sentence-transformers/all-MiniLM-L6-v2/config_1_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.67s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  8.85it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____BAAI/bge-base-en-v1.5/config_1_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.87s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00, 10.25it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Snowflake/snowflake-arctic-embed-m-v1.5/config_2_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.96s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  8.16it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Snowflake/snowflake-arctic-embed-m/config_2_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.96s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.81it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____intfloat/e5-base-v2/config_2_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.83s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.10it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____sentence-transformers/all-MiniLM-L6-v2/config_2_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.73s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.90it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____BAAI/bge-base-en-v1.5/config_2_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  8.31it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Snowflake/snowflake-arctic-embed-m-v1.5/config_3_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.92s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.38it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Snowflake/snowflake-arctic-embed-m/config_3_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:03<00:00,  3.02s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.52it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____intfloat/e5-base-v2/config_3_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.85s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.80it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____sentence-transformers/all-MiniLM-L6-v2/config_3_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.93it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____BAAI/bge-base-en-v1.5/config_3_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.91s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00, 10.03it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Snowflake/snowflake-arctic-embed-m-v1.5/config_4_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.84s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00, 10.17it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Snowflake/snowflake-arctic-embed-m/config_4_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:03<00:00,  3.03s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.83it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____intfloat/e5-base-v2/config_4_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.81s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  8.47it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____sentence-transformers/all-MiniLM-L6-v2/config_4_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.85it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____BAAI/bge-base-en-v1.5/config_4_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.81s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.37it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Snowflake/snowflake-arctic-embed-m-v1.5/config_5_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.99s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.90it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Snowflake/snowflake-arctic-embed-m/config_5_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00, 10.03it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____intfloat/e5-base-v2/config_5_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.98s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.23it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____sentence-transformers/all-MiniLM-L6-v2/config_5_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.77s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.65it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____BAAI/bge-base-en-v1.5/config_5_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  9.75it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Snowflake/snowflake-arctic-embed-m-v1.5/config_6_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:03<00:00,  3.03s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00, 10.20it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Snowflake/snowflake-arctic-embed-m/config_6_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:03<00:00,  3.01s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00, 10.32it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____intfloat/e5-base-v2/config_6_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  8.73it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____sentence-transformers/all-MiniLM-L6-v2/config_6_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.87s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00,  8.31it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____BAAI/bge-base-en-v1.5/config_6_EMB____\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "executing dimension reduction + clustering: 100%|██████████| 1/1 [00:02<00:00,  2.95s/it]\n",
      "calculating_consensus_matrix: 100%|██████████| 16/16 [00:01<00:00, 10.24it/s]\n",
      "calculating_optimal_K:  93%|█████████▎| 28/30 [00:04<00:00,  6.21it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_cols = [col for col in df_recipes_cleaned.columns if col.endswith(\"EMB\")]\n",
    "labels_config = {}\n",
    "\n",
    "for col in emb_cols:\n",
    "    print(f\"____{col}____\")\n",
    "    emnbedding_vector = np.vstack(df_recipes_cleaned[col].values)\n",
    "    all_weights, all_labels = run_clustering_grid(col, emnbedding_vector, UMAP_GRID, CLUSTERING_GRID, OUT_DIR)\n",
    "    best_consensus_labels = concensure_clustering(col, emnbedding_vector, all_weights, all_labels, OUT_DIR) \n",
    "    labels_config[col] = best_consensus_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5028b130",
   "metadata": {},
   "source": [
    "**visualise cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e549ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)\n",
    "embedding_2d = reducer.fit_transform(emnbedding_vector)\n",
    "\n",
    "df_recipes_cleaned['UMAP_1'] = embedding_2d[:, 0]\n",
    "df_recipes_cleaned['UMAP_2'] = embedding_2d[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28060f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_recipes_cleaned, \n",
    "    x='UMAP_1', \n",
    "    y='UMAP_2', \n",
    "    color=best_consensus_labels.astype(str),\n",
    "    hover_data=['NAME_CLEAND'],\n",
    "    title='UMAP projection of recipe embeddings'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
