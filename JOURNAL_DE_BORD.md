# Journal de bord - Pipeline de donn√©es NutriRAG

**Responsable** : Camelia Mazouz  
**P√©riode** : Janvier 2026  
**Composant** : Pipeline de donn√©es et infrastructure Snowflake

---

## üìã Vue d'ensemble

Ce journal documente la conception et la mise en ≈ìuvre compl√®te du pipeline de donn√©es pour le projet NutriRAG, incluant l'ingestion, le nettoyage, la transformation et le stockage de donn√©es de recettes dans Snowflake.

---

## üéØ Responsabilit√©s et organisation d'√©quipe

### Ma contribution
J'ai √©t√© responsable de l'ensemble du **pipeline de donn√©es** (dossier `database/`), couvrant :
- La s√©lection et le sampling des donn√©es brutes
- Le nettoyage et la validation des recettes
- L'automatisation de la cr√©ation de la base de donn√©es
- L'ingestion dans Snowflake
- L'optimisation des performances du pipeline

### Collaboration interne et externe
- **√âquipe RAG** : Transformation des colonnes pour r√©pondre √† leurs besoins sp√©cifiques
  - Stringification des listes (`tags`, `steps`, `ingredients`)
  - Transformation de la colonne `nutrition` en format texte
  - Ajout d'une colonne `filters` pour faciliter la recherche par r√©gimes alimentaires

---

## üìä Phase 0 : Exploration et √©chantillonnage initial

### Contexte et contraintes
**Probl√©matique** : Le dataset complet contient ~230 000 recettes, mais Snowflake impose des limites de taille pour l'upload de fichiers.

**Solution** : S√©lection al√©atoire progressive pour permettre aux autres √©quipes de commencer leur travail.

### √âtapes r√©alis√©es

#### 1. Premier √©chantillon (10 000 recettes)
- **Objectif** : Fournir rapidement un dataset de travail aux autres √©quipes

#### 2. Nettoyage approfondi (√©chantillon 1 000 lignes)

D√©veloppement et validation des r√®gles de nettoyage sur un petit √©chantillon :

**Crit√®res de filtrage appliqu√©s** :
```python
# R√®gles m√©tier
- minutes > 5  # Temps de pr√©paration minimum
- id IS NOT NULL
- submitted IS NOT NULL
- description IS NOT NULL
- name IS NOT NULL AND LENGTH(name) > 0

# Validation des listes
- ARRAY_SIZE(tags) >= 1
- ARRAY_SIZE(nutrition) = 7  # Format standardis√©
- ARRAY_SIZE(steps) >= 1
- ARRAY_SIZE(ingredients) >= 1
- ARRAY_SIZE(filters) >= 1 # colonne ajout√©e
```

**Impl√©mentation** :
- V√©rification de coh√©rence des champs : `n_steps` doit √©galer `len(steps)`, `n_ingredients` doit √©galer `len(ingredients)`
- Parsing s√©curis√© des donn√©es imbriqu√©es (conversion des listes stock√©es en texte en vraies structures Python) et exclusion des lignes avec incoh√©rences
- V√©rification des valeurs n√©gatives (ex: `minutes < 0`) et des champs obligatoires vides
- R√©alis√© dans le notebook de nettoyage (**clean_recipes.ipynb**)


#### 3. Fusion avec donn√©es externes (~200 000 recettes)
(Tache r√©alis√©e avec Aleksandra et Aur√©lien)

**Probl√®me identifi√©** : Les donn√©es de base manquaient de pr√©cision sur les quantit√©s d'ingr√©dients.

**Solution** : Fusion de trois sources de donn√©es
- Donn√©es de base (RAW_recipes.csv)
- Donn√©es Kaggle avec images et m√©tadonn√©es (`behnamrahdari/foodcom-enhanced-recipes-with-images`)
- Donn√©es Kaggle avec quantit√©s et search terms (`shuyangli94/foodcom-recipes-with-search-terms-and-tags`)

**R√©sultat** : ~200 000 recettes enrichies

#### 4. Filtrage par r√©gimes alimentaires (~110 000 recettes)

**Objectif** : Cr√©er une colonne `filters` pour permettre la recherche par crit√®res di√©t√©tiques.

**Tags cibl√©s** :
```
vegetarian, vegan, kosher, egg_free, dairy_free, salt_free, 
flour_less, grain_free, sugar_free, carb_free, low_carb, 
low_cholesterol, low_protein, low_calorie, low_saturated_fat, 
gluten_free, fat_free, no_shell_fish, diabetic, low_sodium, 
nut_free, low_fat, halal, amish, non_alcoholic
```

**Impl√©mentation** : 
- Durant la phase 0, ajout rapide d'une colonne `filters` via un script Python qui parcourt les tags et g√©n√®re les filtres dans la table RAW.
- Dans la version automatis√©e, cr√©ation d'une UDF Snowflake Python pour extraire ces filtres plus rapidement c√¥t√© serveur.

**R√©sultat** : Conservation de ~110 000 recettes avec au moins un filtre di√©t√©tique (repr√©sentatif de la base originale)

#### 5. √âchantillon final (50 000 recettes)

- Nettoyage final avec v√©rification des noms (`LENGTH(name) > 0`)
- S√©lection de 50 000 recettes pour √©quilibrer volume et qualit√©

**Notebooks utilis√©s** :
- **clean_recipes.ipynb** : Nettoyage et validation
- **Ingestion_des_doonees.ipynb** : Ajout de la colonne filters et ingestion

---

## ‚öôÔ∏è Phase 1 : Automatisation de la cr√©ation de la base de donn√©es

### Objectif
Automatiser compl√®tement le setup de l'infrastructure Snowflake pour permettre un d√©ploiement reproductible.

### Architecture d√©velopp√©e

```
database/scripts/
‚îú‚îÄ‚îÄ python/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                   # Point d'entr√©e du pipeline
‚îÇ   ‚îú‚îÄ‚îÄ PipelineOrchestrator.py   # Orchestration des phases
‚îÇ   ‚îú‚îÄ‚îÄ SnowflakeConnector.py     # (-> SnowflakeUtils) Connexion Snowflake
‚îÇ   ‚îú‚îÄ‚îÄ DataLoader.py             # (deleted) T√©l√©chargement Drive/Kaggle
‚îÇ   ‚îú‚îÄ‚îÄ DataTransformer.py        # (not used) Nettoyage et validation
‚îÇ   ‚îú‚îÄ‚îÄ RecipeCleaner.py          # (deleted) Nettoyage et validation
‚îÇ   ‚îú‚îÄ‚îÄ generate_schema.py        # Generation du schema de la DB en se basant sur les variables d'env
‚îÇ   ‚îú‚îÄ‚îÄ SnowFlakeIngestor.py      # (-> CleanData) Ingestion dans Snowflake
‚îÇ   ‚îî‚îÄ‚îÄ config.py                 # Configuration centralis√©e
‚îî‚îÄ‚îÄ sql/
    ‚îú‚îÄ‚îÄ schema_db_template.sql     # Template du sch√©ma
    ‚îú‚îÄ‚îÄ schema_db_generated.sql    # Sch√©ma g√©n√©r√©
    ‚îú‚îÄ‚îÄ nutri_score.sql    # Sch√©ma g√©n√©r√©
    ‚îú‚îÄ‚îÄ ingest_clean_recipes.sql   # Ingestion et nettoyage
    ‚îî‚îÄ‚îÄ extract_filters_udf.sql    (deleted)# UDF extraction filtres
  (Les mentions entre parenth√®ses indiquent l'√©tat final des fichiers : renommage vers un nouvel utilitaire, suppression, ou non utilisation dans la version stabilis√©e.)
```

### Scripts d√©velopp√©s

#### 1. **main.py**
Point d'entr√©e avec options modulaires :
```bash
# Pipeline complet
python main.py

# Phases individuelles
python main.py --setup-only      # Cr√©ation du sch√©ma
python main.py --load-only       # T√©l√©chargement des donn√©es
python main.py --clean-only      # Nettoyage
python main.py --ingest-only     # Ingestion des donn√©es dans Snowflake

# Tests avec donn√©es limit√©es
python main.py --nrows 1000
```

#### 2. **PipelineOrchestrator.py**
Orchestration des 4 √©tapes du pipeline :
- **Etape 0** : Setup du sch√©ma Snowflake
- **Etape 1** : Chargement des donn√©es (Drive + Kaggle)
- **Etape 2** : Nettoyage et transformation
- **Etape 3** : Ingestion dans Snowflake

#### 3. **SnowflakeConnector.py**
Gestion s√©curis√©e de la connexion :
- Authentification par cl√© priv√©e RSA
- M√©thode `safe_execute()` pour l'ex√©cution s√©curis√©e de SQL

#### 4. **DataLoader.py**
T√©l√©chargement automatique des donn√©es :
- Google Drive avec `gdown`
- Kaggle avec `kagglehub`
- Cache local pour √©viter les t√©l√©chargements r√©p√©t√©s

#### 5. G√©n√©ration de sch√©ma dynamique
Template SQL avec variables d'environnement :
```sql
-- schema_db_template.sql
CREATE DATABASE IF NOT EXISTS ${DATABASE_NAME};
CREATE WAREHOUSE IF NOT EXISTS ${WAREHOUSE_NAME};
```

G√©n√©ration automatique via **generate_schema.py** √† partir du `.env`

---

## üîÑ Phase 2 : Strat√©gies d'ingestion des donn√©es

### Approche 1 : G√©n√©ration de requ√™tes INSERT (‚ùå Abandonn√©e)

**Impl√©mentation initiale** : **SqlInsertGenerator.py**
- G√©n√©ration d'un fichier SQL avec des milliers d'INSERT
- Ex√©cution s√©quentielle dans Snowflake

**Probl√®me** : 
- Temps d'ex√©cution > 3 heures
- Gestion complexe des caract√®res sp√©ciaux et √©chappement
- Forte consommation m√©moire

**Raison de l'abandon** : Non-scalable pour notre volume de donn√©es

### Approche 2 : M√©thode native Snowflake (‚úÖ Solution retenue)

**Avantages** :
- Traitement c√¥t√© serveur (beaucoup plus rapide)
- Exploitation des capacit√©s de calcul distribu√©es de Snowflake
- Pas de transfert r√©seau massif
- Temps d'ex√©cution beaucoup plus rapide

**SQL d'ingestion** : tout le traitement est ex√©cut√© c√¥t√© serveur Snowflake. On charge les donn√©es une seule fois, puis on applique UDFs et requ√™tes SQL directement dans Snowflake pour √©viter les transferts r√©seau et acc√©l√©rer l'ex√©cution.

---

## üíæ √âvolution de la strat√©gie de stockage des donn√©es

### √âvolution 1 : T√©l√©chargement √† la demande
**M√©thode initiale** : 
- T√©l√©chargement depuis Kaggle/Drive √† chaque ex√©cution
- Stockage temporaire local

**Probl√®mes** :
- Temps de setup ~15-20 minutes
- D√©pendance √† la connexion internet


### √âvolution 2 : Git LFS (Large File Storage)
**Impl√©mentation actuelle** :
- Stockage des CSV dans le dossier `dataset/`
- Versionnement avec Git LFS
- Fichiers track√©s :
  ```
  dataset/RAW_recipes.csv
  dataset/RAW_interactions.csv
  dataset/recipes_enhanced_v2.csv
  dataset/recipes_w_search_terms.csv
  dataset/cleaned_ingredients.csv
  ```

**Avantages** :
- **Praticit√©** : Clone du repo = donn√©es incluses
- **Reproductibilit√©** : M√™me version pour toute l'√©quipe
- **Rapidit√©** : Pas de t√©l√©chargement √† chaque setup
- **Versionnement** : Historique des modifications de donn√©es

---

## üöÄ Phase 3 : Optimisation et pr√©-calcul

### Strat√©gie d'optimisation

**Constat** : Le pipeline complet (t√©l√©chargement ‚Üí nettoyage ‚Üí transformation ‚Üí ingestion) prenait > 1 heure.

**Solution** : Pr√©-calcul et stockage des donn√©es finales

### Donn√©es pr√©-calcul√©es stock√©es dans Git LFS

1. **Sample final (50K recettes)** : Donn√©es nettoy√©es et valid√©es
2. **Clusters d'ingr√©dients** : **`ingredients_with_clusters.csv`**
3. **Matching ingr√©dients** : Correspondance avec bases nutritionnelles
4. **Parsing des quantit√©s** : Extraction et normalisation des unit√©s

### Processus de chargement direct

#### Comparaison entre phase 1 et 3

```mermaid
graph LR
    subgraph "Phase 1 : Pipeline complet"
        A1["üì• T√©l√©charger<br/>Drive + Kaggle<br/>"] --> B1["üßπ Nettoyer<br/>"]
        B1 --> C1["üîÑ Transformer<br/>"]
        C1 --> D1["üìä Calculer parsing<br/>"]
        D1 --> E1["‚¨ÜÔ∏è Charger<br/>Snowflake<br/>"]
    end
    
    subgraph "Phase 3 : Optimis√©e"
        A3["üìÅ CSV pr√©-calcul√©s<br/>Git LFS<br/>"] --> E3["‚¨ÜÔ∏è Ing√©rer<br/>dans snowflake<br/>"]
    end
    
    style A1 fill:#ff6b6b
    style E1 fill:#ff6b6b
    style A3 fill:#51cf66
    style E3 fill:#51cf66
```

**R√©sultat** : 
- Temps de setup r√©duit de ~60 minutes √† ~5 minutes
- Pipeline simplifi√© et plus fiable
- Facilite l'onboarding de nouveaux d√©veloppeurs

---

## ‚ö†Ô∏è Difficult√©s rencontr√©es et r√©solutions

### Incident critique : Suppression accidentelle du warehouse

**Contexte** : Pendant une phase de tests

**Incident** : 
- Suppression compl√®te du warehouse Snowflake
- **Perte de** :
  - Base de donn√©es compl√®te
  - Toutes les tables (RAW, CLEANED, DEV_SAMPLE)
  - Proc√©dures stock√©es
  - UDFs (User Defined Functions)
  - Agents Snowflake
  - Notebooks de test

**Impact** : 
- Arr√™t temporaire du d√©veloppement (10min de stress)
- Risque de perte de plusieurs jours de travail

**R√©solution** : 
- D√©couverte de la fonctionnalit√© **Time Travel** de Snowflake
- Utilisation de la "corbeille" Snowflake (UNDROP)
- **R√©cup√©ration int√©grale** du warehouse :
  ```sql
  UNDROP DATABASE NUTRIRAG_PROJECT;
  UNDROP WAREHOUSE NUTRIRAG_PROJECT;
  ```

**Le√ßons apprises** :
1. ‚úÖ Toujours travailler avec une base de test s√©par√©e
2. ‚úÖ Documenter les commandes de sauvegarde/restauration
3. ‚úÖ Importance des features de r√©cup√©ration de Snowflake

### Autres difficult√©s techniques

#### 1. Gestion des donn√©es imbriqu√©es
**Probl√®me** : Colonnes avec listes Python (`['item1', 'item2']`) stock√©es en string

**Solution** :
- Parsing avec `ast.literal_eval` dans **DataTransformer.py**
- Conversion en ARRAY natif Snowflake
- Validation du format avant insertion

#### 2. Performance de l'ingestion
**Probl√®me** : INSERT s√©quentiel trop lent

**Solution** : Passage √† l'ingestion c√¥t√© serveur SQL (voir Phase 2)

#### 3. Qualit√© des donn√©es
**Probl√®me** : Nombreuses incoh√©rences (n_steps ‚â† len(steps))

**Solution** :
- Validation stricte dans **DataTransformer.check_consistency()**
- Logs d√©taill√©s des erreurs
- Exclusion des lignes invalides plut√¥t que correction automatique

---

## üìà M√©triques et r√©sultats

### Volume de donn√©es trait√©
- **Dataset initial** : 230 000 recettes
- **Apr√®s fusion** : 200 000 recettes enrichies
- **Apr√®s filtrage di√©t√©tique** : 110 000 recettes
- **√âchantillon final** : 50 000 recettes de haute qualit√©

### Performance du pipeline
| Phase | Temps (version initiale) | Temps (version optimis√©e) |
|-------|--------------------------|---------------------------|
| Setup sch√©ma | 1 min | 1 min |
| T√©l√©chargement | 15 min | 30 sec (Git LFS) |
| Ingestion | 180 min (INSERT) | 8 min (SQL natif) |
| **TOTAL** | **~196 min** | **~9 min** |

**Gain de performance** : R√©duction de 93% du temps d'ex√©cution

### Qualit√© des donn√©es
- **Taux de validation** : ~90% des recettes passent les crit√®res de qualit√©
- **Compl√©tude** : 100% des recettes finales ont tous les champs obligatoires
- **Filtres di√©t√©tiques** : 100% des recettes finales ont au moins un filtre

---

## üõ†Ô∏è Technologies et outils utilis√©s

### Infrastructure
- **Snowflake** : Data warehouse cloud
  - Snowpark pour le traitement Python
  - UDFs Python pour la logique m√©tier
  - Time Travel pour la r√©cup√©ration de donn√©es

### Langages
- **Python 3.9+** : Scripts d'orchestration et transformation
- **SQL** : Requ√™tes Snowflake et d√©finition de sch√©ma

### Outils de d√©veloppement
- **Git LFS** : Versionnement des gros fichiers
- **Jupyter Notebooks** : Exploration et prototypage
- **VS Code** : D√©veloppement principal

---

## üìö Documentation produite

### Fichiers README
- **database/README.md** : Documentation compl√®te du pipeline
- Scripts SQL comment√©s avec docstrings

### Structure de code
- Architecture modulaire avec s√©paration des responsabilit√©s
- Classes d√©di√©es par fonctionnalit√© (Loader, Transformer, Connector, Ingestor)
- Configuration centralis√©e dans **config.py**

### Exemples d'utilisation
Documentation des commandes dans le README principal avec cas d'usage sp√©cifiques

---


## ü§ù Collaboration et communication

### Points de synchronisation
- R√©unions/echanges de messages avec l'√©quipe 1 pour faire des points d'avancement

### M√©thodes de travail GIT
- Chaque fonctionnalit√© est d√©velopp√©e sur une branche d√©di√©e. Une fois termin√©e, une Pull Request est cr√©√©e et assign√©e √† un autre membre de l'√©quipe pour r√©vision et validation avant fusion.

---

## ‚úÖ Conclusion

Le d√©veloppement du pipeline de donn√©es pour NutriRAG a n√©cessit√© plusieurs it√©rations pour atteindre un √©quilibre entre :
- **Performance** : R√©duction drastique des temps d'ex√©cution
- **Qualit√©** : Validation stricte et nettoyage approfondi
- **Maintenabilit√©** : Code modulaire et bien document√©
- **Reproductibilit√©** : Git LFS et automatisation compl√®te

Les choix techniques (SQL natif Snowflake, Git LFS, pr√©-calcul) ont permis de cr√©er un pipeline robuste et scalable, pr√™t pour la production.

