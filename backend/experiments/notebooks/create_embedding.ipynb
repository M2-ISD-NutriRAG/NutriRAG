{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26905175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "original_cwd = os.getcwd()\n",
    "\n",
    "backend_path = os.path.abspath(os.path.join(original_cwd, \"../backend\"))\n",
    "added_backend = False\n",
    "\n",
    "if not any(\"backend\" in p for p in sys.path):\n",
    "    sys.path.insert(0, backend_path)\n",
    "    added_backend = True\n",
    "    print(f\"Added backend to sys.path: {backend_path}\")\n",
    "else:\n",
    "    print(\"Backend already in sys.path, skipping.\")\n",
    "\n",
    "from shared.snowflake.client import SnowflakeClient\n",
    "\n",
    "os.chdir(original_cwd)\n",
    "print(f\"Returned to original working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5e697a",
   "metadata": {},
   "source": [
    "**read the config file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = \"config/base_config.json\"\n",
    "\n",
    "with open(CONFIG_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306de016",
   "metadata": {},
   "source": [
    "**get the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b97fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_RECIPIES_FILE = config['input_recipies_file']\n",
    "df_recipes = pd.DataFrame()\n",
    "\n",
    "#check if the file already exists read it\n",
    "if os.path.exists(INPUT_RECIPIES_FILE):\n",
    "    print(f\"File {INPUT_RECIPIES_FILE} already exists. Skipping save.\")\n",
    "    df_recipes = pd.read_csv(INPUT_RECIPIES_FILE)\n",
    "else:\n",
    "    print(f\"Saving DataFrame to {INPUT_RECIPIES_FILE}\")\n",
    "    client = SnowflakeClient()\n",
    "\n",
    "    conn = client._conn\n",
    "    table = \"RECIPES_SAMPLE_EVAL_EMBEDDING\"\n",
    "\n",
    "    df_recipes = pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
    "\n",
    "    client.close()\n",
    "    \n",
    "    df_recipes.to_csv(INPUT_RECIPIES_FILE, index=False)\n",
    "\n",
    "df_recipes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207db5aa",
   "metadata": {},
   "source": [
    "**define a function that clean the columns used for embedding** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_columns_to_embedd(tag_value: any, col_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Format text of the columns used for embedding\n",
    "\n",
    "    Args:\n",
    "        tag_value (any): The input value to clean. Can be a string, list, number, or None.\n",
    "                        Will be converted to string before processing.\n",
    "        col_name (str): The label/prefix to add before the cleaned text \n",
    "                       (e.g., \"NAME\", \"TAGS\", \"INGREDIENTS\").\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned and formatted text in the format \"{col_name}: {cleaned_text}.\"\n",
    "             Returns empty string if input is None or empty.\n",
    "    \"\"\"\n",
    "    \n",
    "    if tag_value is None or tag_value == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(tag_value)\n",
    "    \n",
    "    # Remove list brackets and quotes\n",
    "    text = re.sub(r\"[\\[\\]'\\\"]\", \"\", text)\n",
    "    \n",
    "    text = text.replace(\"|\", \",\")\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Keep only alphanumeric, spaces, and . , ? !\n",
    "    text = re.sub(r\"[^a-z0-9 .,?!]+\", \"\", text)\n",
    "    \n",
    "    # Remove excess spaces\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    \n",
    "    # Remove spaces before commas\n",
    "    text = re.sub(r\" ,\", \",\", text)\n",
    "    \n",
    "    # Clean up spaces around punctuation\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Return formatted text\n",
    "    return f\"{col_name}: {text}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3e7430",
   "metadata": {},
   "source": [
    "**extract only required columns for embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7b0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract only required columns for embedding\n",
    "COLUMNS_TO_CLEAN = config[\"columns_to_clean\"]\n",
    "\n",
    "for col in COLUMNS_TO_CLEAN:\n",
    "    col_clean_name = COLUMNS_TO_CLEAN[col]['column_name']\n",
    "    start_text = COLUMNS_TO_CLEAN[col]['start_text']\n",
    "\n",
    "    df_recipes[col_clean_name] = df_recipes[col].apply(clean_columns_to_embedd, args=(start_text, ))\n",
    "\n",
    "df_recipes_cleaned = df_recipes[ [col['column_name'] for col in COLUMNS_TO_CLEAN.values()] ]\n",
    "\n",
    "#add the id to keep track of the recepies\n",
    "df_recipes_cleaned['ID'] = df_recipes['ID'] \n",
    "df_recipes_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdcdc5c",
   "metadata": {},
   "source": [
    "**create a column for each combinaison of embedding columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9581dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize a columns for each configuration of columns to embedd\n",
    "COLUMNS_TO_EMBEDDE = config[\"columns_embedding\"]\n",
    "\n",
    "for col_config_name, cols_list in COLUMNS_TO_EMBEDDE.items():\n",
    "    df_recipes_cleaned[col_config_name] = \"\"\n",
    "\n",
    "    for col in cols_list:\n",
    "        column_name_cleaned = COLUMNS_TO_CLEAN[col]['column_name']\n",
    "        df_recipes_cleaned[col_config_name] += df_recipes_cleaned[f\"{column_name_cleaned}\"] + \" \"\n",
    "        \n",
    "df_recipes_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4071057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "df_recipes_cleaned['config_1'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb199cb",
   "metadata": {},
   "source": [
    "**load the embedding models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ed388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the models\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "MODELS_CONFIG = config[\"models\"]\n",
    "\n",
    "#create a dict {name model : model} \n",
    "MODELS_LIST = [SentenceTransformer(model_id) for model_id in MODELS_CONFIG]\n",
    "MODEL_DICT = dict(zip(MODELS_CONFIG, MODELS_LIST))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae4614f",
   "metadata": {},
   "source": [
    "**define a function that count number of token for each config and model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b50a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_size(text: str, model: SentenceTransformer) -> int:\n",
    "    \"\"\"\n",
    "    Compute the number of tokens in the given text using the specified SentenceTransformer model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to tokenize.\n",
    "        model (SentenceTransformer): The SentenceTransformer model used for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of tokens in the input text.\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = model.tokenizer.tokenize(text)\n",
    "    \n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13513d8",
   "metadata": {},
   "source": [
    "**set the experience id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626996ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIENCE_ID = config[\"experiments_specifique_params\"][\"experiment_id\"]\n",
    "\n",
    "print(EXPERIENCE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df253a48",
   "metadata": {},
   "source": [
    "**set folder and file path for embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_EMBEDDING_FOLDER = config[\"output_experiments_dir\"].format(\n",
    "    experiment_id=EXPERIENCE_ID\n",
    ")\n",
    "\n",
    "os.makedirs(OUTPUT_EMBEDDING_FOLDER, exist_ok=True)\n",
    "\n",
    "OUTPUT_EMBEDDING_FILE = config[\"output_recipies_embedding_file\"].format(\n",
    "    experiment_id=EXPERIENCE_ID\n",
    ")\n",
    "\n",
    "print(OUTPUT_EMBEDDING_FOLDER)\n",
    "print(OUTPUT_EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486bae45",
   "metadata": {},
   "source": [
    "**calculate number of token per recipie for each config and model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed981cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "df_recipes_embedding = df_recipes_cleaned.copy()\n",
    "\n",
    "#count number of token for each config and model\n",
    "for col_name, cols_list in COLUMNS_TO_EMBEDDE.items():\n",
    "    for model_id, model in MODEL_DICT.items():\n",
    "\n",
    "        embedding_col = f\"{model_id}/{col_name}_EMB\"\n",
    "        tokens_col = f\"{embedding_col}_NUMBER_TOKEN\" \n",
    "\n",
    "        number_token = []\n",
    "        for text in tqdm(df_recipes_embedding[col_name], desc=f\"count number token {col_name} with {model_id}\"):\n",
    "            num_tokens = compute_token_size(text, model)\n",
    "            number_token.append(num_tokens)\n",
    "\n",
    "        df_recipes_embedding[f\"{embedding_col}_NUMBER_TOKEN\"] = number_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recipes_embedding['intfloat/e5-base-v2/config_1_EMB_NUMBER_TOKEN'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_recipes_embedding[df_recipes_embedding['intfloat/e5-base-v2/config_1_EMB_NUMBER_TOKEN'] > 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea056e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "token_lengths = pd.DataFrame(index=df_recipes.index)\n",
    "\n",
    "for col in COLUMNS_TO_CLEAN:\n",
    "\n",
    "    token_lengths[col + \"_token_len\"] = df_recipes[col].fillna(\"\").astype(str).apply(lambda x: compute_token_size(x, model))\n",
    "\n",
    "for col in COLUMNS_TO_CLEAN:\n",
    "    print(f\"Column '{col}':\")\n",
    "    print(token_lengths[col + \"_token_len\"].describe())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2709cdd4",
   "metadata": {},
   "source": [
    "**define function to calculate embedding of a text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352421f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_embedding(model: SentenceTransformer, texts: list[str]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute normalized embeddings for a list of texts using the specified model.\n",
    "\n",
    "    Args:\n",
    "        model (SentenceTransformer): The pre-trained sentence transformer model to use.\n",
    "        texts (list[str]): A list of input texts to compute embeddings for.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the normalized embeddings for the input texts.\n",
    "    \"\"\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Compute embeddings\n",
    "    embeddings = model.encode(texts, convert_to_tensor=True, device=device)\n",
    "    \n",
    "    # Normalize embeddings to unit length\n",
    "    normalized_embeddings = normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    return normalized_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c033d70",
   "metadata": {},
   "source": [
    "**compute embedding for each config and model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d590e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create embedding cols \n",
    "from tqdm import tqdm  \n",
    "\n",
    "for col_name, cols_list in COLUMNS_TO_EMBEDDE.items():\n",
    "    for model_id, model in MODEL_DICT.items():\n",
    "\n",
    "        embedding_col = f\"{model_id}/{col_name}_EMB\"\n",
    "                \n",
    "        embeddings = []\n",
    "        for text in tqdm(df_recipes_embedding[col_name], desc=f\"Embedding {col_name} with {model_id}\"):\n",
    "            emb = compute_embedding(model, [text])[0].cpu().numpy()                         \n",
    "            embeddings.append(emb)\n",
    "        \n",
    "        # Save embeddings to new column\n",
    "        df_recipes_embedding[embedding_col] = embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e866787",
   "metadata": {},
   "source": [
    "**write the file of embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89217d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_EMBEDDING_FOLDER, exist_ok=True)\n",
    "\n",
    "df_recipes_embedding.to_csv(OUTPUT_EMBEDDING_FILE, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
